## Introduzione al scientific machine learning e ai modelli informati dalla fisica
[00:00] Viene presentata una panoramica sul scientific machine learning, con attenzione al physics machine learning. I Physics-Informed Neural Networks (PINN) integrano apprendimento automatico e leggi fisiche. Si introduce l’idea che, con molti dati ben raccolti, i dati possano contenere la fisica del problema, permettendo di costruire modelli senza esplicitare la fisica. Nella pratica, di solito si hanno dati parziali, per cui diventa necessario incorporare la fisica nel modello.
[00:10] Si delinea l’approccio dei PIN come reti neurali che assumono come input le coordinate spaziali del punto in cui si valuta la soluzione e, se presenti, parametri della soluzione. In 2D, gli input naturali sono $x$ e $y$. Se la soluzione dipende da parametri (ad esempio un raggio che descrive una geometria), tali parametri si includono nell’input. Si definisce un insieme di punti all’interno del dominio, potenzialmente dispersi, che costituiranno la base per la valutazione del residuo della PDE e delle condizioni al contorno.
[00:20] L’idea generale è confrontata con metodi basati sui dati: minimi quadrati, varianti lineari, reti neurali e approcci che creano modelli dai dati o surrogati, che estraggono caratteristiche rilevanti dai dati disponibili. La novità recente è incorporare la fisica nel modello oltre ai dati. Un modello surrogato è una rappresentazione semplificata che cattura le caratteristiche essenziali del fenomeno riducendo la complessità computazionale e i requisiti di dati, mantenendo coerenza con le leggi fisiche.
[00:25] Si illustra operativamente un caso 2D per l’equazione di Laplace: si creano punti nel dominio (training set del dominio) e punti sulla frontiera (training set del bordo) per valutare rispettivamente il residuo della PDE nel dominio e il residuo delle condizioni al contorno sul bordo. Questa struttura consente di integrare la fisica nella fase di addestramento della rete.
[00:40] Si definisce l’equazione del tipo $- \Delta u = f$ come esempio di PDE ellittica. Si sottolinea la necessità di campionare punti sia nel dominio che sulla frontiera per costruire una funzione di perdita che misuri quanto la soluzione approssimata rispetti la PDE e le condizioni al contorno. L’approccio consente di combinare dati sperimentali e vincoli fisici per guidare l’apprendimento.
[00:45] L’obiettivo diventa utilizzare il machine learning non solo per costruire modelli dai dati, ma per creare modelli surrogati in cui la fisica è integrata. L’inclusione esplicita della fisica riduce il fabbisogno di dati grazie ai vincoli imposti da equazioni e condizioni fisiche. Questo crea un ponte tra l’impostazione classica basata su PDE discretizzate (elementi finiti, differenze finite) e l’impostazione di machine learning che richiede molti dati.
[01:10] L’approccio si colloca tra metodi tradizionali e data-driven, combinando i punti di forza di entrambi. Il vantaggio principale è la maggiore affidabilità e la riduzione dei dati necessari: l’embedding della fisica guida l’apprendimento e limita lo spazio delle soluzioni ammissibili. Una prospettiva promettente è anche la scoperta di nuove relazioni fisiche. I PINN concretizzano questa idea: si descrivono i requisiti per progettare un modello PINN.
[01:30] Si evidenzia che i modelli risultanti sono più affidabili perché l’inclusione della fisica restringe lo spazio delle soluzioni. L’obiettivo non si limita a fisiche note; vi è l’ambizione di identificare relazioni fisiche. I PINN sono una declinazione pragmatica di questo quadro, con struttura di rete, funzione di perdita e differenziazione automatica pensate per imporre la fisica.
[01:40] Si contrasta la funzione di perdita dei PIN con quella delle reti classiche: invece di confrontare predizioni con etichette, si valuta il residuo della PDE e delle condizioni al contorno. Se sono disponibili dati sperimentali in alcuni punti, si aggiunge un blocco di perdita per i dati, che misura l’errore tra la soluzione approssimata e le misure.
[01:55] Nei PIN la loss non si basa su label precalcolate su ogni punto, ma su residui di PDE e vincoli di bordo e, se presenti, dati. La struttura della loss è quindi composita, e la sua minimizzazione porta a una soluzione che rispetta simultaneamente fisica e dati.
## Contesto: deep learning e sostituzione dei metodi numerici
[02:00] Il deep learning ha funzionato in campi come computer vision e natural language processing. Qui si mira a risolvere PDE con reti neurali, potenzialmente sostituendo metodi tradizionali come elementi finiti e differenze finite. L’idea è usare la rete per apprendere la mappatura tra input (coordinate e parametri) e output (soluzione $u$) integrando la fisica nell’addestramento.
[02:10] L’integrazione della fisica nella loss consente di utilizzare pochi dati sperimentali o anche nessun dato diretto, affidandosi ai vincoli della PDE e del bordo. Si introduce la definizione breve: physics-informed indica un modello che include termini di perdita costruiti dal residuo della fisica (equazioni e condizioni), anziché unicamente da errori rispetto a misure.
[02:20] La differenza rispetto a tecniche classiche è l’assenza di mesh. I metodi su mesh (differenze finite, elementi finiti, volumi finiti) richiedono generazione della mesh del dominio e operano su tale discretizzazione. L’approccio di deep learning lavora con punti dispersi e sfrutta la differenziazione automatica per calcolare derivate, eliminando la necessità di una discretizzazione spaziale esplicita.
[02:40] Nell’approccio con rete neurale la mesh non è necessaria; strumenti come la differenziazione automatica sono fondamentali. Nei FEM si parte dal dominio, si crea la mesh e si risolve il problema su quella discretizzazione. La differenziazione automatica consente invece di calcolare derivate del modello in modo esatto rispetto alla rappresentazione interna della rete.
[03:00] Se il dominio è parametrico (ad esempio un profilo d’ala descritto da parametri geometrici) e si vuole calcolare il campo di moto per diverse configurazioni, i metodi classici richiedono rigenerare la mesh per ogni istanza. Un PINN addestrato su parametri può apprendere una mappatura continua rispetto ai parametri, rendendo agevole il trattamento di geometrie variabili.
[03:30] Riguardo alla dimensione spaziale, i metodi classici si applicano in 1D, 2D, 3D. Se lo spazio ha dimensione superiore, l’approccio tradizionale diventa difficoltoso. L’adozione dei PINN può mitigare la curse of dimensionality: l’assenza di mesh riduce la dipendenza dalla dimensione del dominio come vincolo computazionale critico, pur rimanendo il costo del training da considerare.
## Struttura della rete neurale e funzione di perdita
[04:00] La rete neurale impiegata è classica: uno strato di input, più hidden layer e uno strato di output. Le funzioni di attivazione sono quelle usuali e non è richiesta una specifica funzione per i PINN. Le incognite del problema sono i pesi e i bias di tutti i layer, che determinano la forma della funzione approssimante.
[04:20] La differenziazione automatica è cruciale: consente di calcolare derivate del modello rispetto agli input o ai parametri per valutare gli operatori differenziali nelle PDE. Il calcolo delle derivate permette di formulare i termini di residuo della PDE direttamente sulla rete, senza discretizzare il dominio in celle o elementi.
[00:55] Si definisce una rete che prende in input $x$, $y$ (eventualmente $t$ e parametri) e produce in output $u$, approssimando la soluzione. Con pesi e bias inizializzati casualmente, l’uscita iniziale è lontana dalla soluzione vera. Si introduce una funzione di perdita per misurare l’accuratezza: nel caso del Laplaciano, il residuo $- \Delta \tilde{u} - f$ viene valutato sui punti nel dominio.
[01:10] La loss include anche un termine che misura la violazione delle condizioni al contorno, ossia il residuo rispetto all’operatore di bordo $B$. La loss complessiva è composta da due blocchi: uno per la PDE nel dominio e uno per il bordo. Questa struttura guida l’ottimizzazione dei pesi della rete verso una soluzione coerente con fisica e vincoli.
[02:10] Poiché la PDE può contenere derivate di ordine superiore, si necessita di un modo per calcolare tali derivate della $\\tilde{u}$ rispetto alle coordinate di input. La differenziazione automatica consente di ottenere, ad esempio, derivate seconde rispetto a $x$ e $y$ per calcolare il Laplaciano, rendendo possibile valutare il residuo in ogni punto campionato.
[02:25] Si calcola quindi $\\Delta \\tilde{u}$ e il residuo $- \\Delta \\tilde{u} - f$. Idealmente, il residuo dovrebbe essere nullo; nelle prime fasi del training non lo è. Si aggiunge il residuo del bordo $B(\\tilde{u})$ per quantificare il rispetto delle condizioni al contorno e, se presenti, iniziali. La loss valuta dunque la soddisfazione della PDE e dei vincoli sul dominio e sui bordi.
## Formalizzazione del problema con PDE e gestione del tempo
[04:40] Si formalizza una PDE generale, assunta per semplicità priva di dipendenza dal tempo. L’idea si estende a problemi dipendenti dal tempo, con PDE, condizioni al contorno e condizione iniziale. La soluzione è indicata con $u$, il punto nel dominio spaziale con $x$, e l’operatore di bordo $B$ descrive condizioni di tipo Dirichlet, Neumann, Robin, periodiche, ecc.
[05:00] Il tempo può essere trattato come componente dell’input. Se una componente, ad esempio $x_1$, rappresenta il tempo, l’operatore di bordo per condizioni iniziali agisce imponendo la soluzione a $t=0$. Questa formulazione astratta comprende condizioni al contorno e, ove necessario, condizioni iniziali per la corretta definizione del problema.
[05:20] Per problemi strettamente dipendenti dal tempo, si aggiunge un termine temporale come $du/dt$ e si includono condizioni iniziali; in equazioni come quella delle onde compaiono derivate seconde rispetto al tempo. Nella PDE compare un parametro $\\lambda$, scalare o vettoriale, che rappresenta proprietà dell’equazione (coefficenti o caratteristiche del mezzo), e che può essere trattato come variabile da stimare o come input.
[05:40] Si richiama l’idea di compromesso tra dati e fisica: con pochi dati, è necessario incorporare molta fisica; con molti dati, il peso dei dati aumenta, ma l’inclusione della fisica resta strategica per guidare l’apprendimento e ridurre l’ambiguità delle soluzioni, fornendo vincoli forti che limitano lo spazio delle funzioni ammissibili.
## Termine dati nella loss e ponderazione dei contributi
[02:40] In una situazione stazionaria in un dominio con sorgente di calore e disponibilità di misure in alcuni punti, i dati da soli possono non bastare. Si introduce un terzo blocco nella loss legato ai dati, minimizzando l’errore (ad esempio quadratico medio) nei punti di misura. Questo integra informazione empirica con i vincoli della fisica.
[02:55] La loss di un PIN include tipicamente tre componenti: residuo della PDE, residuo delle condizioni al contorno (e iniziali, se presenti), residuo sui dati sperimentali (se presenti). La loss può essere definita come combinazione lineare di questi termini con pesi $\\omega_{\\text{PDE}}$, $\\omega_{\\text{BC}}$ e $\\omega_{\\text{DATA}}$. Variando i pesi si bilancia l’importanza della fisica e dei dati a seconda della loro qualità e quantità.
## Normalizzazione, forma non dimensionale e addestramento
[03:10] Nei modelli di reti neurali è buona pratica normalizzare i dati per facilitare l’addestramento. Nei PIN, quando ci si affida alla fisica, la normalizzazione corrisponde all’uso di equazioni in forma non dimensionale. Ad esempio, nelle Navier–Stokes, si adottano grandezze scalate e numeri adimensionali come il Reynolds per ridurre la variabilità numerica.
[03:25] In generale, anche la funzione sorgente $f$ nella PDE è preferibilmente espressa in forma non dimensionale. Questo rende l’addestramento più agevole, diminuendo squilibri di scala tra i termini dell’equazione e facilitando la convergenza degli ottimizzatori durante la minimizzazione della loss.
[03:40] Definita la loss, si procede come nelle reti neurali tradizionali: si minimizza la funzione di perdita per trovare pesi e bias che meglio rappresentano la relazione tra input e output. Minimizzando la loss, la rete funge da modello surrogato della PDE: dato $(x,y)$ (e altri input, se presenti), la rete restituisce $u$.
[03:55] Dopo l’addestramento, risolvere la PDE equivale a valutare la rete, operazione spesso più economica del solve numerico tradizionale. Occorre considerare il costo del training: per PDE complesse, il tempo di addestramento può essere significativo. Il costo totale è dato da “tempo di training + tempo di valutazione”, potenzialmente superiore al costo di una singola soluzione FEM per problemi semplici.
[04:10] I PIN sono efficaci quando si risolve lo stesso problema molte volte, variando parametri (proprietà del fluido, geometria del dominio, ecc.). Se si esplorano migliaia di configurazioni, conviene investire nel training e poi usare la rete come surrogato per valutazioni rapide, invece di migliaia di simulazioni via FEM o differenze finite.
[04:25] La pipeline tipica include tre blocchi: rete neurale che produce $\\tilde{u}$, blocco di differenziazione automatica per le derivate necessarie, blocco di fisica che, tramite la loss, impone PDE, condizioni al contorno e dati se presenti. La loss è minimizzata con metodi di ottimizzazione standard per aggiornare pesi e bias.
## Vantaggi dei PIN e confronto con metodi classici
[04:40] I PIN sono mesh-free: non serve generare una mesh, basta un insieme di punti nel dominio e sul bordo. Questo è vantaggioso in geometrie complesse, dove la generazione della mesh può essere costosa e laboriosa, talvolta più della risoluzione stessa del problema.
[04:50] La differenziazione automatica evita gli errori di troncamento tipici della differenziazione numerica. Nei FEM, la formulazione debole porta a integrali del tipo
$$
\int_{\Omega} \nabla \phi_i \cdot \nabla \phi_j \, d\Omega
$$
che in 2D e 3D sono valutati con quadrature, introducendo errore di quadratura. Nei PIN si evita questa specifica fonte di errore perché le derivate sono calcolate esattamente in base alla rappresentazione del modello.
[05:05] È relativamente semplice applicare i PIN a problemi diretti (forward) e inversi (stima di parametri), e a diverse classi di equazioni (paraboliche, ellittiche, integro-differenziali). Una volta espressa la forma dell’equazione nei termini di loss, gran parte del lavoro concettuale è definito e si procede all’addestramento del modello.
## Limiti e difficoltà: equazioni iperboliche e campionamento
[05:20] Le equazioni iperboliche sono più difficili poiché presentano shock e discontinuità. Con punti distribuiti nel dominio, catturare la posizione della discontinuità o dello shock è complicato. Esistono varianti dei PIN che trattano problemi iperbolici, ma richiedono modifiche rispetto all’impianto base.
[05:35] La gestione di geometrie complesse è naturale nei PIN grazie al “cloud” di punti, ma occorre considerare fonti di errore e aspetti di addestramento e generalizzazione. Il campionamento dei punti e la struttura della rete influenzano significativamente le prestazioni del modello e la qualità della soluzione ottenuta.
[05:50] Collegando i PIN al concetto di rete come approssimatore universale, si nota che questa proprietà è non costruttiva. Si introducono le fonti di errore nei PIN: errore di approssimazione (limitazione dello spazio di funzioni della rete rispetto allo spazio infinito), errore di generalizzazione (dipendenza dal numero finito di punti di campionamento), errore di ottimizzazione (difficoltà nel raggiungere il minimo globale della loss).
[06:05] Fissata un’architettura di rete, si definisce un sottoinsieme di funzioni approssimabili. Si indica con $u_f$ la migliore approssimazione di $u$ nello spazio rappresentabile dalla rete. L’errore di approssimazione misura la qualità con cui la famiglia di funzioni della rete approssima la vera soluzione. Analogamente ai FEM, lo spazio è un sottospazio del dominio funzionale generale.
[06:20] In pratica, l’algoritmo PIN raramente raggiunge esattamente $u_f$, perché intervengono errore di generalizzazione e errore di ottimizzazione. Con un numero finito di punti e un’architettura data, si ottiene al più $u_T$, soluzione ottima rispetto ai punti campionati. L’ottimizzazione numerica può fermarsi in minimi locali o non raggiungere il minimo globale.
[06:35] L’errore globale della soluzione PIN è quindi la somma, o un limite superiore della somma, di tre contributi: errore di approssimazione, errore di generalizzazione, errore di ottimizzazione. Questa scomposizione guida l’analisi pratica delle prestazioni e delle scelte progettuali del modello e del processo di addestramento.
[06:50] A differenza dei FEM, dove esistono stime di convergenza legate all’ordine $p$ dell’elemento e al passo $h$, per i PIN non c’è una teoria consolidata che fornisca limiti superiori o tassi di convergenza per tutti e tre gli errori. Ci sono risultati parziali. Riducendo la dimensione della rete aumenta l’errore di approssimazione; reti troppo grandi possono portare a overfitting.
[07:05] Non si dispone di limiti rigorosi per l’errore globale. Anche quando la PDE ben posta ha soluzione unica, i PIN possono convergere a soluzioni diverse. Le cause includono sensitività all’inizializzazione dei pesi, presenza di minimi locali della loss, distribuzione dei punti di campionamento nel dominio e sul bordo.
[07:20] Una strategia pratica è l’addestramento in ensemble: si addestrano più reti con inizializzazioni o campionamenti diversi e si seleziona la realizzazione migliore. Questo approccio può mitigare la variabilità dovuta a fattori di ottimizzazione e campionamento, migliorando la robustezza della soluzione.
[07:35] Gli iperparametri dei PIN includono quelli delle reti tradizionali (architettura, attivazioni, ottimizzatori, learning rate) e parametri specifici: numero e distribuzione dei punti di residuo nel dominio e sul bordo. Queste scelte influenzano la capacità di approssimazione e la stabilità del training.
[07:45] Confronto sintetico:
- Nei FEM, le funzioni di base sono polinomi; nei PIN, le “basi” sono collegate alle attivazioni non lineari.
- Nei FEM, i parametri del problema discreto sono coefficienti di espansione; nei PIN, i parametri sono pesi e bias.
- Nei FEM, i punti derivano dalla mesh; nei PIN, si usano punti dispersi nel dominio e sul bordo.
- Nei FEM, la PDE si traduce in un sistema lineare da formulazione debole; nei PIN, la fisica è imposta tramite la loss.
- Nei FEM, le fonti di errore includono spazio di approssimazione e quadratura; nei PIN, gli errori sono approssimazione, generalizzazione, ottimizzazione, senza limiti consolidati.
## Estensioni: equazioni integro-differenziali e problemi inversi
[08:00] Per un’equazione integro-differenziale, il lato differenziale si tratta come nelle PDE viste; il lato integrale richiede formule di quadratura per la discretizzazione, introducendo errore di quadratura nella valutazione degli integrali. L’impianto complessivo del PIN rimane lo stesso.
[08:10] La loss continua a includere residui della PDE, del bordo e, se presenti, dei dati. L’ottimizzazione ricerca pesi e bias che minimizzano la perdita totale, integrando i diversi contributi. La presenza di integrali aggiunge una fonte di errore legata alla quadratura.
[08:25] Per problemi inversi con parametro $\\lambda$ da stimare, si considerano misure sperimentali (ad esempio temperatura su una barra) con condizioni al contorno note. La loss include un termine per le misure e si minimizza rispetto ai parametri della rete e a $\\lambda$.
[08:35] In questo modo, si ottiene una stima del parametro coerente con la PDE, le condizioni al contorno e i dati, sfruttando la struttura physics-informed della loss per informare l’ottimizzazione e garantire coerenza con la fisica del problema.
## Distribuzione dei punti, adattività e mini-batch
[08:50] I punti nel dominio possono essere generati casualmente o uniformemente. Questa scelta non sempre è efficiente, specialmente in problemi con discontinuità o strati limite. Se la soluzione presenta un fronte quasi discontinuo, nelle regioni costanti la cattura è semplice, mentre nelle zone con bruschi gradienti un campionamento uniforme può essere insufficiente.
[09:05] Per migliorare, si usano metodi adattativi analoghi ai FEM: si adatta la distribuzione dei punti basandosi sul residuo della PDE, aggiungendo più punti nelle regioni dove il residuo è elevato. Il residuo funge da indicatore delle aree in cui l’approssimazione è carente.
[09:15] Questa strategia si combina con l’uso di mini-batch nei metodi stocastici di ottimizzazione (come SGD, Adam), focalizzando l’aggiornamento dei pesi nelle zone di errore maggiore. Ciò può accelerare la convergenza e migliorare la qualità della soluzione nelle regioni critiche.
## Imposizione delle condizioni al contorno: soft e hard
[09:30] Le condizioni al contorno sono spesso inserite nella loss come residuo dell’operatore di bordo, ossia con imposizione “soft”: si minimizza il residuo, senza garanzia di coincidenza esatta dei valori sul bordo. In pratica, i valori su bordo possono non coincidere perfettamente con quelli imposti.
[09:40] È possibile imporre vincoli “hard” modificando l’architettura per soddisfare esattamente i valori sul bordo. Ad esempio, su $[0,1]$ con $u(0)=0$ e $u(1)=0$, si introduce un termine nella soluzione approssimata $\\hat{u}$ che si annulla ai bordi, come $x(1-x)$, fungendo da “lifting” lineare per imporre l’aderenza sul bordo.
[09:55] Questa tecnica è efficace in situazioni semplici, geometrie non complesse e condizioni omogenee. In condizioni non omogenee, come $u(0)=5$ e $u(1)=8$, l’imposizione hard diventa più complessa. Nella pratica, si preferisce la soft constraint; per migliorare la qualità sui bordi si aumenta il numero di punti di bordo inclusi nella loss, rafforzando il vincolo tramite il campionamento.
## Strumenti software e librerie; conclusioni operative
[10:10] Esistono diverse librerie per implementare PIN. DeepXDE, compatibile con TensorFlow e PyTorch, è semplice da usare, ben documentata e include esempi su problemi diretti, inversi, identificazione di parametri, PDE e ODE. È utilizzabile su Google Colab con supporto GPU (CUDA) e funziona anche su CPU.
[10:20] Altre librerie, incluse quelle sviluppate da NVIDIA, possono essere più performanti ma richiedono hardware CUDA specifico. La scelta dello strumento dipende dalle risorse disponibili e dall’obiettivo: facilità d’uso o massime prestazioni di training.
[10:35] Con questi elementi si conclude la parte teorica. Sono previsti esercizi tratti da prove precedenti, con tempo per impostare le soluzioni e discussione collettiva. Il materiale verrà pubblicato online. È possibile organizzare una sessione aggiuntiva nel pomeriggio di venerdì, subordinata all’esito di un sondaggio.
[10:45] Si propone una pausa di 10 minuti, dopo la quale verranno presentati gli esercizi. L’organizzazione della sessione aggiuntiva online dipenderà dalle preferenze raccolte, con l’obiettivo di approfondire ulteriormente i temi affrontati.