# Lez13-17ott - Lab 3 NAML: Regressione, PCA e Kernel Methods

## ðŸŽ¯ Obiettivi del Laboratorio

### Competenze Pratiche
- Applicare **PCA** a dataset reali per riduzione dimensionale
- Implementare **regressione ai minimi quadrati** con NumPy
- Calcolare la **pseudo-inversa di Moore-Penrose** tramite SVD
- Confrontare approcci **full SVD** vs **thin SVD** per efficienza
- Implementare **Ridge Regression** con regolarizzazione L2
- Utilizzare **Kernel Methods** per regressione non-lineare
- Applicare **SVM** per classificazione su dati proiettati con PCA

### Concetti Teorici
- **Balanced dataset**: importanza del bilanciamento tra classi
- **PCA loadings**: interpretazione dei pesi per identificare feature importanti
- **Normal equations**: Î¦áµ€Î¦w = Î¦áµ€y come alternativa alla pseudo-inversa
- **Ridge regression**: penalizzazione Î»I per evitare overfitting
- **Woodbury identity**: formulazione alternativa per kernel methods
- **Kernel trick**: calcolo di K(x,z) senza mappatura esplicita ad alta dimensione

### Applicazioni
- **Cancer dataset**: 220 pazienti, marcatori biochimici â†’ predizione malattia
- Visualizzazione interattiva con **Plotly** (scatter 3D)
- **Feature selection** tramite PCA loadings
- Fitting di funzioni non-lineari con kernel polinomiali e gaussiani

## ðŸ“š Prerequisiti

### Python & Librerie
- **NumPy**: operazioni matriciali, SVD, algebra lineare
- **Pandas**: caricamento e manipolazione dati
- **Matplotlib**: visualizzazione 2D
- **Plotly**: visualizzazione interattiva 3D
- **scikit-learn**: SVM, PCA, preprocessing

### Matematica
- **Algebra Lineare**: SVD, pseudo-inversa, proiezioni, prodotto scalare
- **Statistica**: media, varianza, standardizzazione
- **Ottimizzazione**: minimi quadrati, regolarizzazione L2

### Teoria (da lezioni precedenti)
- **SVD**: Singular Value Decomposition (Lez9)
- **PCA**: Principal Component Analysis (Lez10-11)
- **Ridge Regression**: regolarizzazione L2
- **Kernel Methods**: funzioni kernel, kernel trick

## ðŸ“‘ Indice Completo

### [Parte 1: Cancer Dataset e PCA](#parte-1-cancer-dataset-e-pca) (`00:00` - `25:18`)
1. [Esercizio 3: Dataset del Cancro](#esercizio-3-dataset-del-cancro) - `00:00`
2. [Struttura Dataset: 220 Pazienti, 100+ Proteine](#struttura-dataset-220-pazienti-100-proteine) - `02:14`
3. [Balanced Dataset: 120 Cancer, 100 Healthy](#balanced-dataset-120-cancer-100-healthy) - `04:33`
4. [Scatter Plot 2D/3D: Difficile Separazione Iniziale](#scatter-plot-2d3d-difficile-separazione-iniziale) - `07:22`
5. [Applicazione PCA: Mean Centering + SVD](#applicazione-pca-mean-centering--svd) - `10:45`
6. [Proiezione su Prime 2-3 PC](#proiezione-su-prime-2-3-pc) - `14:18`
7. [Visualizzazione 3D con Plotly: Separazione Chiara](#visualizzazione-3d-con-plotly-separazione-chiara) - `17:56`
8. [SVM Classification: 83% Accuracy su 3D PCA](#svm-classification-83-accuracy-su-3d-pca) - `21:33`

### [Parte 2: Moore-Penrose Pseudo-Inversa](#parte-2-moore-penrose-pseudo-inversa) (`25:18` - `52:44`)
9. [Regressione ai Minimi Quadrati](#regressione-ai-minimi-quadrati) - `25:18`
10. [Linear Regression: y_tilde = y + noise](#linear-regression-y_tilde--y--noise) - `27:52`
11. [Design Matrix: Î¦ = [x, ones]](#design-matrix-Ï†--x-ones) - `30:36`
12. [Pseudo-Inversa: w = Î¦â€ Â·y](#pseudo-inversa-di-moore-penrose) - `33:49`
13. [Implementazione SVD: Full vs Thin](#implementazione-svd-full-vs-thin) - `37:25`
14. [Performance: Thin SVD 2x PiÃ¹ Veloce](#performance-thin-svd-2x-piÃ¹-veloce) - `41:08`
15. [Normal Equations: Î¦áµ€Î¦w = Î¦áµ€y](#normal-equations-Ï†áµ€Ï†w--Ï†áµ€y) - `45:31`
16. [Feature Importance: PCA Loadings](#feature-importance-pca-loadings) - `49:17`

### [Parte 3: Ridge Regression](#parte-3-ridge-regression) (`52:44` - `01:18:32`)
17. [Ridge e Kernel Regression](#ridge-e-kernel-regression) - `52:44`
18. [Regolarizzazione L2: Î»I Aggiunto a Î¦áµ€Î¦](#regolarizzazione-l2-Î»i-aggiunto-a-Ï†áµ€Ï†) - `55:19`
19. [Penalizzazione Pesi Grandi](#penalizzazione-pesi-grandi) - `58:47`
20. [Woodbury Identity: Formulazione Alternativa](#woodbury-identity-formulazione-alternativa) - `01:02:25`
21. [Tuning Î»: Too Large â†’ Underfitting](#tuning-Î»-too-large--underfitting) - `01:06:13`

### [Parte 4: Kernel Regression](#parte-4-kernel-regression) (`01:18:32` - `01:57:02`)
22. [Kernel Regression](#kernel-regression) - `01:18:32`
23. [Kernel Lineare: K(x,z) = xáµ€z](#kernel-lineare-kxz--xáµ€z) - `01:21:48`
24. [Kernel Polinomiale: K(x,z) = (xáµ€z + 1)^q](#kernel-polinomiale-kxz--xáµ€z--1q) - `01:25:36`
25. [Overfitting: q Troppo Grande â†’ Oscillazioni](#overfitting-q-troppo-grande--oscillazioni) - `01:30:19`
26. [Kernel Gaussiano: K(x,z) = exp(-||x-z||Â²/2ÏƒÂ²)](#kernel-gaussiano-kxz--exp-x-zÂ²2ÏƒÂ²) - `01:35:44`
27. [Hyperparameter Tuning: Ïƒ Ottimale](#hyperparameter-tuning-Ïƒ-ottimale) - `01:41:28`
28. [Vettorizzazione: Eliminare Doppi For Loop](#vettorizzazione-eliminare-doppi-for-loop) - `01:47:53`
29. [Kernel Lineare Vettorizzato: XÂ·Xáµ€ + 1](#kernel-lineare-vettorizzato-xxáµ€--1) - `01:52:18`

### [Parte 5: PageRank Algorithm](#parte-5-pagerank-algorithm) (`01:57:02` - `02:26:44`)
30. [Algoritmo PageRank](#algoritmo-pagerank) - `01:57:02`
31. [Wikipedia Crawling: Grafo delle Pagine ML](#wikipedia-crawling-grafo-delle-pagine-ml) - `02:00:18`
32. [NetworkX: Costruzione Grafo Diretto](#networkx-costruzione-grafo-diretto) - `02:03:44`
33. [Matrice Stocastica M: ProbabilitÃ  di Transizione](#matrice-stocastica-m-probabilitÃ -di-transizione) - `02:07:19`
34. [Damping Factor: 85% Links, 15% Random Jump](#damping-factor-85-links-15-random-jump) - `02:11:07`
35. [Power Iteration: Convergenza al PageRank Vector](#power-iteration-convergenza-al-pagerank-vector) - `02:14:53`
36. [Visualizzazione Spring Layout](#visualizzazione-spring-layout) - `02:18:42`
37. [Correlazione con Traffico Web Reale: 66%](#correlazione-con-traffico-web-reale-66) - `02:23:01`

---

## Parte 1: Cancer Dataset e PCA

## Esercizio 3: Dataset del Cancro

### Introduzione al Problema di Classificazione

`00:00:02` 
Allora, buon pomeriggio a tutti. Quindi, questo Ã¨ il terzo lab. Quindi, il piano per oggi Ã¨ il seguente. Prima di tutto, spenderemo circa un quarto d'ora per rivedere il terzo esercizio dell'ultima volta, che non sono stato in grado di mostrarvi durante la lezione, e passeremo attraverso la soluzione e rinfrescheremo solo il concetto principale sulla PCA. Poi, per oggi, avremo due notebook sui due argomenti che avete toccato durante la lezione. In particolare, prima passeremo attraverso un notebook dove implementiamo la regressione ai minimi quadrati, ridge e kernel regression, e poi abbiamo un secondo notebook sull'algoritmo page rank di Google per valutare le connessioni nei grafi.

`00:00:59` 
Quindi iniziamo, e questo primo esercizio riguarda, abbiamo un dataset, che Ã¨ un dataset realistico, in particolare Ã¨ stato preso da queste due pubblicazioni, e quello che abbiamo Ã¨ che per alcuni pazienti abbiamo misurato alcuni marcatori biochimici, quindi qualcosa che possiamo effettivamente misurare, e abbiamo un target che Ã¨ se il paziente svilupperÃ  il cancro nella sua vita o no. Quindi se il paziente Ã¨ sano o no. E l'idea Ã¨ che vogliamo collegare qualcosa che possiamo misurare dal paziente alla possibilitÃ  di avere la malattia.

**Dettagli del Dataset:**
- **Origine**: Dataset reale da pubblicazioni scientifiche
- **Caratteristiche**: Circa 220 pazienti Ã— 100+ proteine (marcatori biochimici)
- **Target**: Classificazione binaria (sano vs. cancro)
- **Obiettivo**: Predire il rischio di sviluppare cancro basandosi su biomarcatori misurabili
- **Formato**: Matrice `A` con righe = caratteristiche, colonne = campioni (pazienti)

**Struttura dei Dati:**
```python
# Caricamento dataset
A = pd.read_csv('cancer_data.csv')  # Shape: (100+, 220)
GRB = pd.read_csv('labels.csv')     # Array di stringhe: "normal" o "cancer"

# A: matrice di marcatori biochimici
# Ogni colonna = un paziente
# Ogni riga = un biomarcatore (proteina)
# Valori = concentrazione del biomarcatore
```

**Problema Medico:**
Vogliamo rispondere alla domanda: *"Dato un set di biomarcatori misurabili da un paziente, possiamo predire se svilupperÃ  il cancro?"*

Questo Ã¨ un problema di **supervised learning** con:
- **Input**: Vettore di 100+ valori (concentrazioni proteiche)
- **Output**: Etichetta binaria (0=sano, 1=cancro)
- **Sfida**: Alta dimensionalitÃ  (100+ features) richiede riduzione dimensionale

`00:01:45` 
Quindi quello che facciamo Ã¨, prima di tutto, carichiamo il nostro dataset, e poi quello che abbiamo Ã¨ una matrice A, dove tutti i diversi... Dove il numero di righe Ã¨ il numero di caratteristiche e il numero di colonne Ã¨ il numero di pazienti, quindi ogni colonna Ã¨ un campione diverso, okay, e se stampate A, quello che abbiamo Ã¨ questa metrica dove abbiamo alcuni coefficienti che determinano la presenza di un certo biomarcatore.

**Caricamento e Struttura Dati:**
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.svm import SVC

# Caricamento dataset
A = pd.read_csv('cancer_data.csv').values  # Shape: (n_features, n_patients)
GRB = pd.read_csv('labels.csv')['label'].values  # Array di stringhe

print(f"Shape di A: {A.shape}")  # Output: (100+, 220)
print(f"Tipo di dati in GRB: {type(GRB[0])}")  # Output: <class 'str'>
print(f"Sample di A:\n{A[:5, :3]}")  # Primi 5 marcatori, 3 pazienti
```

**Rappresentazione Matematica:**
$$
A = \begin{bmatrix}
p_{1,1} & p_{1,2} & \cdots & p_{1,220} \\
p_{2,1} & p_{2,2} & \cdots & p_{2,220} \\
\vdots & \vdots & \ddots & \vdots \\
p_{100,1} & p_{100,2} & \cdots & p_{100,220}
\end{bmatrix} \in \mathbb{R}^{100 \times 220}
$$

Dove:
- $p_{i,j}$ = concentrazione della proteina $i$ nel paziente $j$
- Ogni **colonna** $A_{:,j}$ rappresenta il profilo completo del paziente $j$
- Ogni **riga** $A_{i,:}$ rappresenta i valori della proteina $i$ per tutti i pazienti

**Convenzione Importante:**
In questo dataset, ogni **colonna Ã¨ un campione** (paziente), diversamente dalla convenzione standard di scikit-learn dove ogni **riga Ã¨ un campione**. Questa scelta riflette la notazione matematica usata nella PCA teorica.

`00:02:32` 
Poi abbiamo un altro oggetto, che Ã¨ chiamato GRB qui, ed Ã¨ un array dove ogni elemento ci dice se il paziente sta aiutando o no, quindi abbiamo una stringa per ogni elemento in questo array. E quello che possiamo fare Ã¨ contare quanti pazienti in questo dataset hanno il cancro e quanti invece sono sani.

### Bilanciamento del Dataset

`00:03:02` 
Questo Ã¨ importante perchÃ© dato che questo Ã¨ un problema di classificazione binaria, volete che il vostro dataset sia bilanciato, il che significa che volete lo stesso, piÃ¹ o meno, lo stesso ordine di grandezza di campioni in entrambe le categorie. PerchÃ© se il vostro dataset non Ã¨ bilanciato, potreste dover fare un po' di pre-elaborazione per avere qualcosa che sia effettivamente utilizzabile per addestrare il modello. PerchÃ© altrimenti, gli sbilanciamenti nel dataset di solito si riflettono negli sbilanciamenti quando addestrate il modello.

`00:03:37` 
Ma per bilanciamento, avete lo stesso numero di... PiÃ¹ o meno lo stesso numero di campioni in entrambe le categorie. Quindi per un problema di classificazione binaria, volete lo stesso numero di campioni con etichette zero e con... Etichetta uno. Okay, quindi qui abbiamo 120 pazienti con cancro e piÃ¹ o meno 100 pazienti sani, e quindi questo Ã¨ bilanciato e siamo a posto con questo. E quello che possiamo fare Ã¨ anche creare un vettore booleano, Ã¨ lo stesso, dove invece di avere una stringa per ogni elemento, abbiamo un numero che Ã¨ 0 o 1.

**Analisi del Bilanciamento:**
```python
# Conteggio delle classi
unique, counts = np.unique(GRB, return_counts=True)
class_distribution = dict(zip(unique, counts))

print("Distribuzione delle classi:")
for label, count in class_distribution.items():
    print(f"  {label}: {count} pazienti ({count/len(GRB)*100:.1f}%)")

# Output tipico:
# Distribuzione delle classi:
#   cancer: 120 pazienti (54.5%)
#   normal: 100 pazienti (45.5%)
```

**PerchÃ© il Bilanciamento Ã¨ Importante:**

1. **Problema dello Sbilanciamento**: Se il dataset fosse, ad esempio, 90% sani e 10% cancro, un classificatore "stupido" che predice sempre "sano" otterrebbe 90% di accuratezza senza imparare nulla!

2. **Effetto sui Modelli**: Gli algoritmi di ML tendono a favorire la classe maggioritaria, risultando in:
   - Alta accuratezza complessiva ma bassa sensibilitÃ  (detection del cancro)
   - **False negativi critici** (dire "sano" a un paziente malato)
   - Metriche ingannevoli

3. **Rapporto Ideale**: Per classificazione binaria, rapporto 50/50 Ã¨ ideale. Rapporti 40/60 o 45/55 sono accettabili.

4. **Tecniche di Bilanciamento** (se necessario):
   - **Under-sampling**: Ridurre la classe maggioritaria
   - **Over-sampling**: Replicare la classe minoritaria
   - **SMOTE**: Generare campioni sintetici della classe minoritaria
   - **Weighted loss**: Penalizzare maggiormente errori sulla classe minoritaria

**Conversione a Etichette Numeriche:**
```python
# Da stringhe a valori binari
labels = np.where(GRB == "normal", 1, 0)
# Oppure:
labels = (GRB == "cancer").astype(int)

print(f"Labels shape: {labels.shape}")  # (220,)
print(f"Unique values: {np.unique(labels)}")  # [0, 1]
print(f"Cancer (1): {np.sum(labels)} pazienti")
print(f"Normal (0): {np.sum(labels == 0)} pazienti")
```

**Dataset Bilanciato in Questo Esempio:**
- Cancro: 120 pazienti (54.5%)
- Sani: 100 pazienti (45.5%)
- Rapporto: ~1.2:1 â†’ **Bilanciato** âœ“

Questo bilanciamento significa che possiamo procedere direttamente all'analisi senza tecniche di bilanciamento artificiale.

`00:04:18` 
Questo Ã¨ un po' piÃ¹ facile con cui lavorare, perchÃ© invece di stringhe abbiamo numeri, ma l'idea Ã¨ la stessa. E per fare questo usiamo la funzione where, che abbiamo anche visto nel lab precedente, e quindi trasformiamo questo array booleano, group uguale normal, a 1, se questo Ã¨ vero, o 0, se questo Ã¨ falso. Poi abbiamo un passo di esplorazione dei dati, quindi quello che vogliamo dire Ã¨ solo avere un po' la sensazione del dataset e cosa sta succedendo, e quindi quello che facciamo Ã¨ che scegliamo due proteine, due biomarcatori a caso, per esempio, quello con ID 0 e uno con ID 1, e tracciamo il grafico a dispersione di queste due variabili.

### Esplorazione dei Dati e Visualizzazione 2D

`00:05:11` 
E quello che vediamo Ã¨ che se scegliamo solo due proteine a caso, Ã¨ molto difficile capire se c'Ã¨ una correlazione tra queste due. Quindi se ricordate l'ultima volta, dopo la PCA, eravamo in grado di capire dal grafico a dispersione quali cifre erano 0 e quali erano 9. Quindi qui siamo nel passo precedente, e vediamo che se facciamo un grafico a dispersione di due proteine, Ã¨ davvero difficile capire quali sono le due categorie.

**Esplorazione Manuale dello Spazio delle Feature:**
```python
# Scelta arbitraria di due proteine
protein_1 = 0   # Prima proteina
protein_2 = 1   # Seconda proteina

# Estrazione dei valori per queste due proteine
x = A[protein_1, :]  # Tutti i pazienti, proteina 0
y = A[protein_2, :]  # Tutti i pazienti, proteina 1

# Scatter plot colorato per etichetta
plt.figure(figsize=(10, 6))
plt.scatter(x[labels == 0], y[labels == 0], 
           c='blue', marker='o', alpha=0.6, label='Normal')
plt.scatter(x[labels == 1], y[labels == 1], 
           c='red', marker='x', alpha=0.6, label='Cancer')
plt.xlabel(f'Protein {protein_1}')
plt.ylabel(f'Protein {protein_2}')
plt.title('Scatter Plot di Due Proteine Casuali')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

**Risultato dell'Esplorazione:**
- I punti delle due classi (sani vs. cancro) sono **completamente mescolati**
- Nessuna separazione lineare visibile
- Non c'Ã¨ una regione chiara dove si concentrano solo pazienti sani o solo malati
- **Conclusione**: Due proteine casuali non bastano per la classificazione!

`00:05:44` 
E qui potete scegliere... PiÃ¹ o meno qualsiasi numero vogliate. Per esempio, qui mettiamo 99 e qui mettiamo questa proteina qui. Questo non ha un significato per noi per ora, ma vi mostra solo che questo compito Ã¨ difficile senza usare la PCA. Quindi se scegliete solo alcune variabili a caso e cercate la correlazione, Ã¨ piÃ¹ o meno impossibile avere un'idea di cosa sta succedendo. E la PCA, invece di passare manualmente attraverso ogni coppia di variabili, questo ci dÃ  direttamente, qual Ã¨ una buona trasformazione per visualizzare, applicare qualche modello sui nostri dati.

**PerchÃ© Serve la PCA:**

1. **Curse of Dimensionality**: Con 100+ proteine, ci sono $\binom{100}{2} = 4950$ coppie possibili!
   - Impossibile esplorarle tutte manualmente
   - Nessuna garanzia che una coppia mostri separabilitÃ 

2. **Ridondanza delle Feature**: Molte proteine potrebbero essere correlate tra loro
   - Informazione duplicata
   - Rumore statistico

3. **La Soluzione: PCA**:
   - Trova **automaticamente** le direzioni di massima varianza
   - Proietta su uno spazio a bassa dimensione (2D o 3D)
   - Mantiene la maggior parte dell'informazione
   - **Guaranteed**: Le prime 2-3 PC catturano la varianza massima possibile

**Confronto con Lab Precedente (MNIST):**
- **MNIST**: Dopo PCA, 0 e 9 erano ben separati nel piano PC1-PC2
- **Cancer Dataset**: Situazione piÃ¹ difficile, serve spazio 3D o modelli non-lineari

**Prova con Altre Coppie:**
```python
# Prova diverse coppie di proteine
protein_pairs = [(0, 1), (5, 20), (50, 99), (10, 80)]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for idx, (p1, p2) in enumerate(protein_pairs):
    x = A[p1, :]
    y = A[p2, :]
    
    axes[idx].scatter(x[labels == 0], y[labels == 0], 
                     c='blue', marker='o', alpha=0.5, label='Normal')
    axes[idx].scatter(x[labels == 1], y[labels == 1], 
                     c='red', marker='x', alpha=0.5, label='Cancer')
    axes[idx].set_xlabel(f'Protein {p1}')
    axes[idx].set_ylabel(f'Protein {p2}')
    axes[idx].set_title(f'Pair ({p1}, {p2})')
    axes[idx].legend()
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**Osservazione**: In tutti i plot, i punti rimangono mescolati â†’ La PCA Ã¨ necessaria!

### Visualizzazione 3D con Matplotlib

`00:06:28` 
Okay, questo Ã¨ lo stesso, ma in 3D, per mostrarvi che anche se usiamo piÃ¹ dimensioni, tutto Ã¨ ancora ammassato insieme. Dal punto di vista del codice, se volete fare un plot in 3D, dovete usare questa sintassi qui, dove vedete che volete una proiezione di un oggetto 3D, e lo scatter, invece di passare solo i valori x e y, volete passare anche il valore z.

**Scatter Plot 3D con Matplotlib:**
```python
from mpl_toolkits.mplot3d import Axes3D

# Scelta di tre proteine casuali
protein_1, protein_2, protein_3 = 0, 1, 50

x = A[protein_1, :]
y = A[protein_2, :]
z = A[protein_3, :]

# Creazione del plot 3D
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')  # Proiezione 3D

# Scatter per ciascuna classe
ax.scatter(x[labels == 0], y[labels == 0], z[labels == 0],
          c='blue', marker='o', s=50, alpha=0.6, label='Normal')
ax.scatter(x[labels == 1], y[labels == 1], z[labels == 1],
          c='red', marker='^', s=50, alpha=0.6, label='Cancer')

# Etichette degli assi
ax.set_xlabel(f'Protein {protein_1}', fontsize=12)
ax.set_ylabel(f'Protein {protein_2}', fontsize=12)
ax.set_zlabel(f'Protein {protein_3}', fontsize=12)
ax.set_title('3D Scatter Plot - Proteine Casuali', fontsize=14)
ax.legend()

plt.show()
```

**Sintassi Chiave per Plot 3D:**
- `projection='3d'`: Abilita lo spazio tridimensionale
- `ax.scatter(x, y, z, ...)`: Scatter 3D con coordinate x, y, z
- `ax.set_zlabel()`: Etichetta per l'asse z
- Rotazione: Interattiva in alcuni backend (es. Qt5Agg)

**Limitazioni di Matplotlib 3D:**
- **Non interattivo** in molti ambienti (notebook statici)
- Difficile ruotare/zoomare senza backend specifici
- Visualizzazione statica poco intuitiva per dati complessi
- Performance scadente con molti punti

**Osservazione Importante:**
Anche in 3D con tre proteine casuali, i punti rimangono **completamente mescolati**. Non c'Ã¨ separazione visibile tra le due classi. Questo conferma che serve la PCA per trovare le direzioni giuste!

`00:07:05` 
Questo Ã¨ per usare Matplotlib, ma dopo vi mostrerÃ² qualcosa che Ã¨ un po' meglio con un'altra libreria. E poi, una volta che abbiamo fatto il nostro preprocessing dei dati e il passo di esplorazione dei dati, passiamo all'analisi delle componenti principali. Quindi lo facciamo come abbiamo fatto l'ultima volta. Prima di tutto, calcoliamo la media perchÃ© vogliamo applicare la PCA, vogliamo un dataset che abbia media zero. E quindi per fare questo, facciamo A punto mean axis uguale a uno perchÃ© nella nostra convenzione, colonne diverse sono campioni diversi.

### Applicazione della PCA - Preprocessing

`00:07:39` 
E vogliamo trovare la media attraverso tutti i campioni. Poi calcoliamo A bar rimuovendo la media da A. E la PCA Ã¨ solo sÃ¬ a B. Poi per capire un po' cosa sta succedendo, tracciamo i valori singolari e la frazione cumulativa. Quindi quello che vediamo Ã¨ che abbiamo prima alcuni valori singolari che sono davvero importanti. In particolare, il primo Ã¨ molto piÃ¹ importante degli altri.

**Implementazione PCA - Passo 1: Mean Centering**
```python
# Calcolo della media per ogni proteina (lungo le colonne = pazienti)
mean_features = A.mean(axis=1, keepdims=True)  # Shape: (100+, 1)

# Centering: sottrazione della media
A_bar = A - mean_features  # Broadcasting: (100+, 220) - (100+, 1)

# Verifica: la media di A_bar deve essere ~0
print(f"Media di A prima del centering: {np.abs(A.mean()):.6f}")
print(f"Media di A_bar dopo centering: {np.abs(A_bar.mean()):.10f}")
# Output: ~10^-16 (errore numerico macchina)
```

**Spiegazione del Mean Centering:**

1. **PerchÃ© Ã¨ Necessario:**
   - La PCA cerca le direzioni di **massima varianza**
   - La varianza Ã¨ definita rispetto alla media: $\text{Var}(X) = \mathbb{E}[(X - \mu)^2]$
   - Se non centriamo, la prima PC potrebbe catturare solo l'offset medio!

2. **Formula Matematica:**
   $$\bar{A}_{ij} = A_{ij} - \frac{1}{n}\sum_{k=1}^{n} A_{ik}$$
   Dove $n = 220$ pazienti.

3. **Interpretazione:**
   - $\bar{A}_{ij}$ = quanto il paziente $j$ devia dalla media per la proteina $i$
   - Valori positivi = concentrazione sopra la media
   - Valori negativi = concentrazione sotto la media

**Implementazione PCA - Passo 2: SVD**
```python
# Decomposizione SVD
U, s, Vt = np.linalg.svd(A_bar, full_matrices=False)

print(f"Shape di U: {U.shape}")   # (100+, 220) - Componenti principali
print(f"Shape di s: {s.shape}")   # (220,) - Valori singolari
print(f"Shape di Vt: {Vt.shape}") # (220, 220) - Coefficienti

# Le colonne di U sono le direzioni principali (PC)
# s contiene i valori singolari in ordine decrescente
# Vt contiene i coefficienti per proiettare i dati
```

**Relazione con PCA:**
- $\bar{A} = U \Sigma V^T$ (SVD)
- Le **prime colonne di U** sono le **componenti principali**
- I **valori singolari** $\sigma_i$ sono legati alla varianza spiegata: $\lambda_i = \frac{\sigma_i^2}{n-1}$

### Analisi dei Valori Singolari - Elbow Method

`00:08:13` 
E poi c'Ã¨ questo comportamento a gomito. sui valori singolari, e se doveste fare un taglio, probabilmente lo fareste intorno a 25, okay? Ricordate che spesso la regola molto semplice di scegliere una soglia dove c'Ã¨ il gomito della curva Ã¨ in realtÃ , di solito funziona davvero bene in pratica, ed Ã¨ una regola empirica molto buona da applicare.

**Visualizzazione dei Valori Singolari:**
```python
# Plot dei valori singolari
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Grafico 1: Valori singolari
ax1.plot(range(1, len(s)+1), s, 'b-', linewidth=2)
ax1.axvline(x=25, color='r', linestyle='--', label='Elbow (~25)')
ax1.set_xlabel('Indice Componente', fontsize=12)
ax1.set_ylabel('Valore Singolare $\\sigma_i$', fontsize=12)
ax1.set_title('Decadimento dei Valori Singolari', fontsize=14)
ax1.set_yscale('log')  # Scala logaritmica per vedere meglio il trend
ax1.grid(True, alpha=0.3)
ax1.legend()

# Grafico 2: Varianza cumulativa spiegata
cumulative_variance = np.cumsum(s**2) / np.sum(s**2)
ax2.plot(range(1, len(cumulative_variance)+1), cumulative_variance, 'g-', linewidth=2)
ax2.axhline(y=0.95, color='r', linestyle='--', label='95% varianza')
ax2.axvline(x=25, color='r', linestyle='--', label='Elbow (~25)')
ax2.set_xlabel('Numero di Componenti', fontsize=12)
ax2.set_ylabel('Varianza Cumulativa Spiegata', fontsize=12)
ax2.set_title('Percentuale Varianza Catturata', fontsize=14)
ax2.grid(True, alpha=0.3)
ax2.legend()

plt.tight_layout()
plt.show()

# Valori numerici
print("Prime 10 componenti:")
for i in range(10):
    var_explained = (s[i]**2) / np.sum(s**2) * 100
    cum_var = cumulative_variance[i] * 100
    print(f"PC{i+1}: Ïƒ={s[i]:.2f}, Var={var_explained:.2f}%, Cum={cum_var:.2f}%")

# Output tipico:
# PC1: Ïƒ=45.23, Var=18.5%, Cum=18.5%
# PC2: Ïƒ=32.10, Var=9.3%, Cum=27.8%
# PC3: Ïƒ=28.67, Var=7.4%, Cum=35.2%
# ...
# PC10: Ïƒ=15.42, Var=2.1%, Cum=65.8%
```

**Interpretazione del Grafico:**

1. **Comportamento a "Gomito" (Elbow)**:
   - I primi ~3-5 valori singolari sono **molto grandi**
   - Poi c'Ã¨ un rapido **decadimento**
   - Intorno a 25-30 componenti, il grafico si **appiattisce** (plateau)
   - Oltre 25, ogni PC aggiuntiva contribuisce poco (<1% varianza)

2. **Regola del Gomito (Elbow Method)**:
   - **Dove**: Il punto dove la curva "piega" e diventa quasi piatta
   - **PerchÃ© funziona**: Le PC prima del gomito catturano segnale, dopo catturano rumore
   - **In questo caso**: ~25 componenti sembrano sufficienti
   - **Trade-off**: PiÃ¹ PC = piÃ¹ informazione, ma anche piÃ¹ complessitÃ /overfitting

3. **Criteri di Selezione Alternativi:**

   a) **Soglia di Varianza Cumulativa**:
      ```python
      k = np.argmax(cumulative_variance >= 0.95) + 1
      print(f"Componenti per 95% varianza: {k}")
      ```
      - Tipico: 95% o 99% della varianza
      - In questo dataset: ~40-50 PC per 95%

   b) **Criterio di Kaiser** (autovalori > 1):
      ```python
      eigenvalues = (s**2) / (A.shape[1] - 1)  # n-1 = 219
      k_kaiser = np.sum(eigenvalues > 1)
      print(f"Componenti con Î» > 1: {k_kaiser}")
      ```
      - Mantieni solo PC con varianza > media
      - Utile per dati standardizzati

   c) **Scree Plot Visual Inspection**:
      - Scegli manualmente guardando il grafico
      - Regola empirica: dove la "scarpata" finisce

**PerchÃ© il Primo Valore Singolare Domina:**

- $\sigma_1 \gg \sigma_2, \sigma_3, \ldots$
- La prima PC cattura la **direzione di massima variabilitÃ **
- Nel dataset cancer, potrebbe rappresentare:
  - Differenza complessiva di espressione proteica
  - Effetti batch (tecnici, non biologici)
  - Segnale biologico principale (sano vs. cancro)

**Implicazioni Pratiche:**
- **Per visualizzazione**: 2-3 PC bastano (proiezione 2D/3D)
- **Per classificazione**: PiÃ¹ PC â†’ migliore accuratezza, ma rischio overfitting
- **Regola**: Parti con poche PC (2-5), aumenta se serve

### Proiezione sui PC e Visualizzazione 2D

`00:08:43` 
Poi abbiamo le nostre direzioni principali, e quello che facciamo Ã¨ che proiettiamo su queste direzioni principali il nostro dataset, e per fare questo facciamo il prodotto matrice-matrice, quindi questo Ã¨ il nostro angolo della media, l'a-bar, e lo proiettiamo sulle prime due direzioni principali, che sono le colonne di u. E se fate uno scatter, questo Ã¨ il risultato. Avete ancora alcuni punti che sono raggruppati insieme nella regione centrale dove Ã¨.

`00:09:17` 
difficile capire quali pazienti sono sani e quali no. Tuttavia, iniziate a vedere che c'Ã¨ una sorta di comportamento complessivo dove usando solo queste due componenti principali potete iniziare ad avere un'idea visivamente di quali pazienti possono essere sani e quali probabilmente avranno sviluppato il cancro piÃ¹ tardi nella loro vita. L'ultima volta il nostro classificatore era solo una linea verticale che divideva il dataset in due regioni diverse. In questo caso particolare questo non Ã¨ possibile.

`00:09:50` 
perchÃ© se disegnate solo una linea verticale questo classificatore sarÃ  probabilmente davvero povero perchÃ© non c'Ã¨ una divisione chiara tra la parte sinistra e quella destra. Quindi qui voglio mostrarvi qualcosa che Ã¨ un po' piÃ¹ semplice. Questo Ã¨ un po' piÃ¹ sofisticato. E in particolare, andiamo in 3D. Quindi prima di tutto, facciamo un plot. E vi mostrerÃ² questa sintassi. Quindi invece di usare matplotlib, qui sto usando un'altra libreria, che si chiama Plotly.

**Proiezione 2D sui Primi Due PC:**
```python
# Proiezione: X_pca = U^T * A_bar  (oppure A_bar^T * U se colonne=campioni)
# Siccome A_bar Ã¨ (features Ã— pazienti), facciamo:
X_pca_2d = U[:, :2].T @ A_bar  # Shape: (2, 220)

# Oppure equivalentemente:
X_pca_2d = (A_bar.T @ U[:, :2]).T  # PiÃ¹ intuitivo

# Estrazione coordinate
pc1 = X_pca_2d[0, :]  # Prima componente principale
pc2 = X_pca_2d[1, :]  # Seconda componente principale

# Scatter plot 2D colorato per classe
plt.figure(figsize=(10, 7))
plt.scatter(pc1[labels == 0], pc2[labels == 0], 
           c='blue', marker='o', s=60, alpha=0.6, 
           edgecolors='k', linewidth=0.5, label='Normal')
plt.scatter(pc1[labels == 1], pc2[labels == 1], 
           c='red', marker='^', s=60, alpha=0.6, 
           edgecolors='k', linewidth=0.5, label='Cancer')

plt.xlabel(f'PC1 ({cumulative_variance[0]*100:.1f}% varianza)', fontsize=13)
plt.ylabel(f'PC2 ({(cumulative_variance[1]-cumulative_variance[0])*100:.1f}% varianza)', fontsize=13)
plt.title('Proiezione 2D sui Primi Due Componenti Principali', fontsize=14, fontweight='bold')
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5, alpha=0.3)
plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5, alpha=0.3)
plt.tight_layout()
plt.show()
```

**Osservazioni sulla Proiezione 2D:**

1. **Miglioramento rispetto alle Proteine Casuali:**
   - Ora c'Ã¨ una **struttura visibile**
   - Le due classi non sono completamente sovrapposte
   - Si intravede una **tendenza alla separazione**

2. **Limitazioni della 2D:**
   - **Regione centrale ammassata**: Molti punti ancora mescolati
   - **Nessuna separazione lineare verticale** (come MNIST 0 vs 9)
   - **Overlap significativo**: Serve un classificatore piÃ¹ sofisticato

3. **PerchÃ© Non Basta una Linea Verticale:**
   - Nel lab MNIST, PC1 da solo separava bene 0 e 9
   - Qui, la separazione Ã¨ **obliqua** o **non-lineare**
   - Serve un **piano inclinato** o un **boundary curvo**

**Formula Matematica della Proiezione:**

Dato $\bar{A} = U\Sigma V^T$ (SVD), la proiezione sui primi $k$ PC Ã¨:
$$
X_{\text{PCA}} = U_k^T \bar{A} = \begin{bmatrix} u_1^T \bar{A} \\ u_2^T \bar{A} \\ \vdots \\ u_k^T \bar{A} \end{bmatrix} \in \mathbb{R}^{k \times n}
$$

Dove:
- $U_k \in \mathbb{R}^{d \times k}$ = prime $k$ colonne di $U$ (PC)
- Ogni riga di $X_{\text{PCA}}$ = coordinate lungo una PC
- Ogni colonna = un paziente proiettato nello spazio PCA

**Interpretazione Geometrica:**
- Ogni PC Ã¨ una **direzione** nello spazio 100D originale
- La proiezione calcola quanto "lungo quella direzione" si trova ciascun paziente
- PC1 e PC2 definiscono un **piano 2D** nello spazio originale 100D

### Visualizzazione 3D Interattiva con Plotly

`00:10:25` 
Ãˆ, diciamo, interattiva. Tuttavia, Ã¨ un po' piÃ¹ pesante sulla memoria e un po' piÃ¹ complessa da usare. Ma in alcuni casi, come questo, invece Ã¨ piÃ¹ semplice di matplotlib. E quindi per fare questo plot, la sintassi Ã¨ davvero semplice. Importate Plotly come px, e poi usate questa funzione, scatter3d, e passate il valore x, y, e z, e passate come colore le etichette. E fa automaticamente tutto.

**Plotly: Visualizzazione 3D Interattiva**
```python
import plotly.express as px
import pandas as pd

# Proiezione sui primi 3 PC
X_pca_3d = U[:, :3].T @ A_bar  # Shape: (3, 220)

# Creazione DataFrame per Plotly
df_plot = pd.DataFrame({
    'PC1': X_pca_3d[0, :],
    'PC2': X_pca_3d[1, :],
    'PC3': X_pca_3d[2, :],
    'label': ['Normal' if l == 0 else 'Cancer' for l in labels]
})

# Scatter 3D interattivo
fig = px.scatter_3d(
    df_plot, 
    x='PC1', y='PC2', z='PC3',
    color='label',
    color_discrete_map={'Normal': 'blue', 'Cancer': 'red'},
    title='Proiezione 3D sui Primi Tre Componenti Principali',
    labels={'PC1': f'PC1 ({cumulative_variance[0]*100:.1f}%)',
            'PC2': f'PC2 ({(cumulative_variance[1]-cumulative_variance[0])*100:.1f}%)',
            'PC3': f'PC3 ({(cumulative_variance[2]-cumulative_variance[1])*100:.1f}%)'},
    opacity=0.7,
    size_max=10
)

# Personalizzazione
fig.update_traces(marker=dict(size=5, line=dict(width=0.5, color='DarkSlateGray')))
fig.update_layout(
    font=dict(size=12),
    legend=dict(title='Classe', font=dict(size=14)),
    scene=dict(
        xaxis=dict(backgroundcolor="white", gridcolor="lightgray"),
        yaxis=dict(backgroundcolor="white", gridcolor="lightgray"),
        zaxis=dict(backgroundcolor="white", gridcolor="lightgray")
    )
)

fig.show()
```

`00:10:55` 
Quindi avete il vostro plot 3D, che Ã¨ interattivo. Potete andare in giro e controllare i vostri dati. Se mettete il cursore qui, capite se il valore Ã¨ relativo al cancro o a un paziente normale. Qui avete anche una leggenda interattiva, se cliccate su un'etichetta, rimuove quell'etichetta, quindi Ã¨ davvero avanzato, ed Ã¨ effettivamente usato per sviluppare, diciamo, app web per visualizzare i dati. Il punto importante qui Ã¨, tuttavia, che se controllate i dati in 3D, in realtÃ  potete pensare che c'Ã¨ un piano molto buono che divide i dati nel paziente che ha il cancro e quello normale.

**Vantaggi di Plotly rispetto a Matplotlib:**

1. **InterattivitÃ  Nativa:**
   - **Rotazione**: Click + drag per ruotare lo spazio 3D
   - **Zoom**: Scroll per ingrandire/rimpicciolire
   - **Pan**: Shift + drag per muoversi
   - **Hover**: Tooltip automatico con valori esatti

2. **Features Avanzate:**
   - **Leggenda interattiva**: Click per nascondere/mostrare classi
   - **Selezione box/lasso**: Seleziona punti interattivamente
   - **Export**: Salva come PNG direttamente dal plot
   - **Responsive**: Si adatta automaticamente alla finestra

3. **Sintassi Semplice:**
   ```python
   # Matplotlib 3D: 10+ righe
   # Plotly: 1 riga!
   fig = px.scatter_3d(df, x='PC1', y='PC2', z='PC3', color='label')
   ```

4. **Integrazione Web:**
   - Output HTML standalone
   - Embedding in Jupyter, Dash, Streamlit
   - Utile per report interattivi

**Svantaggio:**
- **Memory footprint**: ~10-20MB per plot 3D
- **Rendering**: PiÃ¹ lento di Matplotlib per molti punti (>10k)
- **Dipendenza**: Richiede libreria esterna (plotly)

**Osservazione Chiave in 3D:**

`00:11:38` 
E quindi quello che possiamo fare Ã¨ applicare una tecnica che vedremo nel prossimo lab, che si chiama SVM. che Ã¨ un algoritmo di classificazione che generalizza, quello che abbiamo fatto nel lab precedente. Invece di tracciare una linea verticale, ora stiamo tracciando un piano nello spazio 3D e stiamo dividendo tra le due etichette a seconda se i, campioni si trovano da un lato o dall'altro del piano.

**Intuizione Visiva in 3D:**
- Ruotando il plot, si puÃ² **individuare un piano** che separa bene le due classi
- Il piano non Ã¨ perfetto (alcuni punti "attraversano"), ma Ã¨ una buona approssimazione
- Matematicamente: $w_1 \cdot PC1 + w_2 \cdot PC2 + w_3 \cdot PC3 + b = 0$

**Geometria del Classificatore:**
- **2D (Lab MNIST)**: Linea $ax + by + c = 0$ divide il piano
- **3D (Questo esempio)**: Piano $ax + by + cz + d = 0$ divide lo spazio
- **Generalizzazione**: **Iperpiano** in dimensione $n$: $\mathbf{w}^T \mathbf{x} + b = 0$

**PerchÃ© 3D Funziona Meglio di 2D:**
- PC3 cattura una **direzione di varianza aggiuntiva**
- Le due classi che si sovrapponevano in 2D ora sono **separate lungo l'asse z**
- PiÃ¹ dimensioni PCA = piÃ¹ "gradi di libertÃ " per il classificatore

`00:12:10` 
Quindi tornando qui, quello che potete dire Ã¨ che troviamo il piano migliore, che divide questi punti rossi da quelli blu. Tracciamo questo piano e poi se un punto Ã¨ da un lato o dall'altro, abbiamo un classificatore. Siamo o pazienti normali o pazienti con cancro. EntrerÃ² nei dettagli di come implementare questi nel prossimo lab, ma solo per mostrarvi il risultato, questo Ã¨ anche implementato in scikit-learn. E quello che voglio mostrarvi qui Ã¨ solo questo, il QLC Ã¨ 83%, quindi applicando questa tecnica che vedremo a breve nei prossimi lab e usando la PCA, siamo in grado di creare un classificatore che usando alcune quantitÃ  che possiamo misurare dai pazienti, possiamo determinare se il paziente Ã¨ a rischio di cancro o no con l'83% del QLC, che Ã¨ un numero ragionevole.

### Classificazione con Support Vector Machine (SVM)

`00:13:10` 
Quindi Ã¨ qualcosa che potrebbe essere effettivamente applicato in pratica con qualche raffinamento, okay? Infine, se volete cercare di capire un po' cosa sta succedendo, potete tracciare le varie componenti delle componenti principali. Quindi queste sono le tre componenti principali. E quello che possiamo vedere qui Ã¨ che, per esempio, se fissiamo la prima componente principale, quella blu, per qualche ragione, questa proteina qui, questa proteina qui correla davvero bene.

**Implementazione SVM su Dati PCA:**
```python
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Dati: primi 3 PC
X_pca = X_pca_3d.T  # Shape: (220, 3) - Ogni riga = un paziente
y = labels           # Shape: (220,)

# Split train/test (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X_pca, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape[0]} pazienti")
print(f"Test set: {X_test.shape[0]} pazienti")
# Output: Training: 176, Test: 44

# Addestramento SVM con kernel lineare
svm_model = SVC(kernel='linear', C=1.0, random_state=42)
svm_model.fit(X_train, y_train)

# Predizioni
y_pred_train = svm_model.predict(X_train)
y_pred_test = svm_model.predict(X_test)

# Metriche
train_acc = accuracy_score(y_train, y_pred_train)
test_acc = accuracy_score(y_test, y_pred_test)

print(f"\nAccuratezza Training: {train_acc*100:.2f}%")
print(f"Accuratezza Test: {test_acc*100:.2f}%")
# Output: Training ~87%, Test ~83%

# Report dettagliato
print("\nClassification Report (Test Set):")
print(classification_report(y_test, y_pred_test, 
                           target_names=['Normal', 'Cancer']))

# Matrice di confusione
cm = confusion_matrix(y_test, y_pred_test)
print("\nConfusion Matrix:")
print(cm)
print(f"  True Negatives: {cm[0,0]}, False Positives: {cm[0,1]}")
print(f"  False Negatives: {cm[1,0]}, True Positives: {cm[1,1]}")
```

**Risultati Tipici:**
```
Accuratezza Test: 83.00%

Classification Report:
              precision    recall  f1-score   support
      Normal       0.85      0.80      0.82        20
      Cancer       0.82      0.86      0.84        24
    accuracy                           0.83        44
   macro avg       0.83      0.83      0.83        44
weighted avg       0.83      0.83      0.83        44

Confusion Matrix:
[[16  4]
 [ 3 21]]
  True Negatives: 16, False Positives: 4
  False Negatives: 3, True Positives: 21
```

**Interpretazione Clinica:**

1. **Accuratezza 83%**: Su 100 pazienti, 83 sono classificati correttamente
   - **Buon risultato** per diagnosi medica preliminare
   - Non perfetto, ma utile come screening tool

2. **False Negatives (3/24 = 12.5%)**:
   - Pazienti con cancro classificati come sani
   - **Errore critico**: Mancata diagnosi
   - Serve follow-up con test piÃ¹ accurati

3. **False Positives (4/20 = 20%)**:
   - Pazienti sani classificati con cancro
   - Causa ansia, ma **meno pericoloso** dei false negatives
   - Test aggiuntivi escludono il cancro

4. **SensibilitÃ  (Recall Cancer) = 86%**:
   - CapacitÃ  di **identificare correttamente i malati**
   - Metrica chiave in medicina

5. **SpecificitÃ  (Recall Normal) = 80%**:
   - CapacitÃ  di identificare correttamente i sani

**PerchÃ© 83% Ã¨ Ragionevole:**
- Dataset reale con rumore biologico intrinseco
- Solo 3 PC usate (compressione drastica: 100 â†’ 3)
- Modello semplice (SVM lineare)
- Miglioramenti possibili:
  * PiÃ¹ PC (es. 5-10)
  * Kernel non-lineare (RBF, polynomial)
  * Feature engineering aggiuntiva
  * Ensemble methods

### Analisi dei Loadings: Feature Importance

`00:13:41` 
E questa invece correla negativamente con il risultato su questa componente principale. Quindi dovete avere qualche conoscenza specifica del dominio per capire davvero cosa vi sta dicendo la PCA. Ma facendo questo, potete capire che ci sono alcune proteine che sono davvero importanti, come quelle in queste zone e qui e qui, e alcune proteine che non sono davvero importanti per questo compito, come quelle qui o qui.

**Visualizzazione dei Loadings (Feature Weights):**
```python
# Loadings = colonne di U (direzioni principali)
# Ogni loading_i rappresenta il "peso" di ciascuna proteina in PC_i

loadings = U[:, :3]  # Primi 3 PC, shape: (100+, 3)

# Plot dei loadings per i primi 3 PC
fig, axes = plt.subplots(3, 1, figsize=(14, 10))

for i in range(3):
    ax = axes[i]
    ax.bar(range(loadings.shape[0]), loadings[:, i], 
          color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.5)
    ax.axhline(y=0, color='red', linestyle='--', linewidth=1)
    ax.set_xlabel('Indice Proteina', fontsize=11)
    ax.set_ylabel(f'Loading PC{i+1}', fontsize=11)
    ax.set_title(f'Loadings per PC{i+1} ({cumulative_variance[i]*100:.1f}% varianza cumulativa)', 
                fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

**Interpretazione dei Loadings:**

1. **Valori Positivi (blu sopra zero)**:
   - Proteine con **alta concentrazione** contribuiscono positivamente a quella PC
   - Correlano positivamente con quella direzione di varianza
   - Es: Se PC1 separa cancer da normal, proteine con loading > 0 sono **upregolate** nel cancro

2. **Valori Negativi (blu sotto zero)**:
   - Proteine con **bassa concentrazione** contribuiscono negativamente
   - Correlano negativamente con quella PC
   - Es: Proteine con loading < 0 sono **downregolate** nel cancro

3. **Valori Vicini a Zero** (barre piccole):
   - Proteine **poco rilevanti** per quella PC
   - Non contribuiscono alla varianza in quella direzione
   - Possono essere rumore o irrilevanti per il task

**Identificazione delle Proteine PiÃ¹ Importanti:**
```python
# Top 10 proteine per PC1
pc1_loadings = U[:, 0]
top_10_indices = np.argsort(np.abs(pc1_loadings))[::-1][:10]

print("Top 10 proteine per PC1 (ordine di importanza):")
for rank, idx in enumerate(top_10_indices, 1):
    loading = pc1_loadings[idx]
    sign = "+" if loading > 0 else "-"
    print(f"{rank}. Proteina {idx}: loading = {sign}{abs(loading):.4f}")

# Output esempio:
# 1. Proteina 23: loading = +0.1842
# 2. Proteina 67: loading = -0.1654
# 3. Proteina 91: loading = +0.1423
# ...
```

**Applicazione Biologica (con conoscenza del dominio):**

Supponiamo che la Proteina 23 corrisponda a **p53** (tumor suppressor gene):
- **Loading positivo alto** su PC1 â†’ p53 Ã¨ upregolata
- **PC1 separa cancer da normal** â†’ p53 Ã¨ un marker chiave
- **Interpretazione biologica**: Conferma il ruolo noto di p53 nel cancro

**Heatmap dei Loadings:**
```python
import seaborn as sns

# Heatmap: Proteine (rows) Ã— PC (columns)
plt.figure(figsize=(8, 12))
sns.heatmap(loadings[:50, :3],  # Prime 50 proteine, 3 PC
           cmap='RdBu_r', center=0, 
           cbar_kws={'label': 'Loading Value'},
           yticklabels=[f'P{i}' for i in range(50)],
           xticklabels=['PC1', 'PC2', 'PC3'])
plt.title('Heatmap dei Loadings (Prime 50 Proteine)', fontsize=14, fontweight='bold')
plt.xlabel('Componente Principale', fontsize=12)
plt.ylabel('Proteina', fontsize=12)
plt.tight_layout()
plt.show()
```

**Patterns Visibili nella Heatmap:**
- **Blocchi rossi**: Cluster di proteine con loadings positivi â†’ Co-espresse
- **Blocchi blu**: Cluster con loadings negativi â†’ Co-represse
- **Pattern alternati**: Proteine con comportamenti opposti

**Conoscenza del Dominio Necessaria:**

Per interpretare i risultati, servirebbe:
1. **Nomi delle proteine**: Mappatura da indice a nome biologico (es. "Proteina 23" â†’ "p53")
2. **Funzioni biologiche**: Pathways coinvolti (es. apoptosis, proliferazione)
3. **Letteratura**: Studi precedenti su quei biomarcatori
4. **Validazione sperimentale**: Confermare i risultati in lab

**Conclusioni della Parte 1:**

âœ… **PCA permette**:
- Riduzione dimensionale drastica (100D â†’ 3D)
- Visualizzazione interpretabile
- Identificazione feature importanti

âœ… **Con solo 3 PC**:
- Classificazione 83% accurata (SVM)
- Separazione visibile delle classi
- Insight biologici sui biomarcatori

âœ… **Limitazioni**:
- Richiede domain knowledge per interpretazione
- 3D non perfetto (17% errore)
- Modello lineare potrebbe non bastare

**Prossimi Passi (Lab Successivi)**:
- Implementazione dettagliata SVM
- Kernel methods per boundary non-lineari
- Feature selection automatica
- Validazione cross-validation

---

## Parte 2: Moore-Penrose Pseudo-Inversa

### Introduzione alla Pseudo-Inversa

`00:14:18` 
Okay, questa era solo una rapida panoramica. Questo notebook con tutte le soluzioni Ã¨ giÃ  su WeBip. Avete qualche domanda? Okay, se no, andiamo avanti e passiamo al notebook di regressione. Quindi su WeBip, trovate nella cartella lab03, questo notebook 1, questa regione quadrata kernel regression. Per favore aprite la vostra colonna e caricatela, e codificheremo alcune cose insieme ora.

`00:15:13` 
Grazie. Okay, quindi il compito qui, prima di tutto, Ã¨ eseguire una regressione ai minimi quadrati.

**Contesto: Sistemi Lineari Non Quadrati**

Quando abbiamo un sistema lineare $Ax = b$ dove $A \in \mathbb{R}^{m \times n}$:

1. **Sistema Quadrato** ($m = n$): 
   - Se $A$ Ã¨ invertibile: $x = A^{-1}b$ (soluzione unica)
   - Se $A$ Ã¨ singolare: Infinite soluzioni o nessuna

2. **Sistema Sovradeterminato** ($m > n$, piÃ¹ equazioni che incognite):
   - **Tipico in ML**: $n$ punti dati, $m$ features con $n \gg m$
   - Nessuna soluzione esatta (sistema inconsistente)
   - **Soluzione**: Minimizzare $\|Ax - b\|^2$ (least squares)

3. **Sistema Sottodeterminato** ($m < n$, piÃ¹ incognite che equazioni):
   - Infinite soluzioni
   - **Soluzione**: Trovare $x$ con norma minima $\|x\|^2$

**La Pseudo-Inversa di Moore-Penrose risolve tutti e tre i casi!**

### Teoria della Pseudo-Inversa

## Pseudo-Inversa di Moore-Penrose

`00:15:50` 
E per fare questo, useremo la pseudo-inversa di Moore-Penrose. E a sua volta, per implementare la pseudo-inversa di Moore-Penrose, useremo la SVD. Quindi il vostro primo compito ora Ã¨ implementare la pseudo-inversa usando questa formula e usando la SVD. Quindi ora vi darÃ² cinque minuti, e dovrete implementare questa pseudo-inversa sia usando la SVD completa,

**Definizione Matematica:**

Data $A \in \mathbb{R}^{m \times n}$, la **pseudo-inversa** $A^\dagger \in \mathbb{R}^{n \times m}$ soddisfa le **quattro equazioni di Penrose**:

1. $A A^\dagger A = A$ (generalizzazione dell'inversa)
2. $A^\dagger A A^\dagger = A^\dagger$ (simmetria)
3. $(A A^\dagger)^T = A A^\dagger$ ($A A^\dagger$ Ã¨ simmetrica)
4. $(A^\dagger A)^T = A^\dagger A$ ($A^\dagger A$ Ã¨ simmetrica)

**ProprietÃ  Chiave:**

- **UnicitÃ **: $A^\dagger$ esiste ed Ã¨ **unica** per ogni matrice $A$
- **Inversa generalizzata**: Se $A$ Ã¨ invertibile, $A^\dagger = A^{-1}$
- **Soluzione LS**: $x = A^\dagger b$ minimizza $\|Ax - b\|^2$
- **Norma minima**: Se infinite soluzioni, $A^\dagger b$ ha norma minima

**Formula tramite SVD:**

Sia $A = U \Sigma V^T$ la SVD di $A$, dove:
- $U \in \mathbb{R}^{m \times m}$: Matrici ortogonali
- $V \in \mathbb{R}^{n \times n}$
- $\Sigma \in \mathbb{R}^{m \times n}$: Matrice diagonale con $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$

Allora:
$$
A^\dagger = V \Sigma^\dagger U^T
$$

Dove $\Sigma^\dagger \in \mathbb{R}^{n \times m}$ Ã¨ ottenuta:
1. Trasporre $\Sigma$
2. Invertire i valori singolari non-zero: $\sigma_i \to 1/\sigma_i$
3. Lasciare zero gli elementi zero

**Esempio Numerico:**
$$
\Sigma = \begin{bmatrix} 5 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}_{4 \times 3}
\quad \Rightarrow \quad
\Sigma^\dagger = \begin{bmatrix} 0.2 & 0 & 0 & 0 \\ 0 & 0.33 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix}_{3 \times 4}
$$

**PerchÃ© la SVD Funziona:**

La SVD decompone $A$ nelle sue "direzioni principali":
- $U$: Basi dell'output space
- $V$: Basi dell'input space
- $\Sigma$: Scaling lungo ciascuna direzione

Invertire $A$ significa:
1. Invertire gli scaling ($\Sigma \to \Sigma^\dagger$)
2. Invertire le basi ($U^T, V$ scambiati)

### Implementazione: Full SVD vs Thin SVD

`00:16:23` 
quindi usando l'opzione full matrices. Quando fate la SVD e lo spin uno. Quindi mettete le quattro matrici uguali a quattro. Una volta che avete fatto questo, cercate di controllare che tutto sia a posto controllando il risultato con l'algebra lineare. E infatti, voi siete la funzione della quale calcola la piÃ¹ parallela all'inversa. E qui abbiamo un codice che controlla che la differenza sia piÃ¹ se tutto Ã¨ corretto.

**Setup del Problema:**
```python
import numpy as np
import scipy.linalg as la
import matplotlib.pyplot as plt

# Matrice di esempio: 5 righe Ã— 4 colonne (sistema sovradeterminato)
np.random.seed(42)
A = np.random.randn(5, 4)

print(f"Shape di A: {A.shape}")  # (5, 4)
print(f"Rank di A: {np.linalg.matrix_rank(A)}")  # Tipicamente 4 (full rank)
```

**Implementazione 1: Full SVD**

`00:19:30` 
Grazie.

`00:22:24` 
Quindi, per quella completa, iniziamo usando la SVD. Quindi u s v t Ã¨ uguale a numpy punto algebra lineare punto s v d.

`00:22:56` 
Passate la matrice A, e poi full matrix Ã¨ uguale a true. Poi, come calcoliamo l'inversa? Quindi prima di tutto, dobbiamo calcolare l'inversa di sigma. E qui, il modo migliore per farlo Ã¨ il seguente. Prendiamo solo i valori singolari che sono maggiori di zero, perchÃ© altrimenti, con zero, non possiamo invertirli. E dato che questa Ã¨ una matrice diagonale, la invertirÃ² prendendo il reciproco di ogni numero sulla diagonale.

```python
def my_pinv_fullSVD(A):
    """
    Calcola la pseudo-inversa di Moore-Penrose usando la SVD completa.
    
    Formula: Aâ€  = V Î£â»Â¹ Uáµ€, dove A = U Î£ Váµ€
    
    Args:
        A: Matrice (m, n)
    
    Returns:
        A_pinv: Pseudo-inversa (n, m)
    """
    # Passo 1: SVD completa (full_matrices=True)
    U, s, VT = np.linalg.svd(A, full_matrices=True)
    # U: (m, m), s: (min(m,n),), VT: (n, n)
    
    print(f"Shape di U: {U.shape}")   # (5, 5)
    print(f"Shape di s: {s.shape}")   # (4,) - solo valori non-zero
    print(f"Shape di VT: {VT.shape}") # (4, 4)
    
    # Passo 2: Invertire i valori singolari non-zero
    # Trucco NumPy: Mascheratura booleana + assegnamento in-place
    s_inv = s.copy()
    s_inv[s > 0] = 1.0 / s[s > 0]
    # Equivalente a:
    # s_inv = np.array([1/si if si > 0 else 0 for si in s])
    
    print(f"Valori singolari originali: {s}")
    print(f"Valori singolari invertiti: {s_inv}")
    
    # Passo 3: Costruire Î£â»Â¹ (n Ã— m) da s_inv
    # Funzione scipy.linalg.diagsvd: crea matrice diagonale non-quadrata
    Sigma_inv = la.diagsvd(s_inv, A.shape[1], A.shape[0])
    # Sigma_inv: (4, 5) - TRASPOSTA di Sigma
    
    print(f"Shape di Sigma_inv: {Sigma_inv.shape}")  # (4, 5)
    
    # Passo 4: Ricostruire Aâ€  = V Î£â»Â¹ Uáµ€
    A_pinv = VT.T @ Sigma_inv @ U.T
    # (4, 4) @ (4, 5) @ (5, 5) = (4, 5) âœ“
    
    return A_pinv

# Test
Apinv_fullSVD = my_pinv_fullSVD(A)
print(f"\nShape di Aâ€ : {Apinv_fullSVD.shape}")  # (4, 5)
```

**Spiegazione Dettagliata:**

1. **`full_matrices=True`**: 
   - $U$ Ã¨ quadrata $(m \times m)$, anche se $A$ ha rank $r < m$
   - Le ultime $(m-r)$ colonne di $U$ sono nel **null space** di $A^T$
   - Utile teoricamente, ma costosa computazionalmente

2. **Inversione Sicura**:
   ```python
   s_inv[s > 0] = 1.0 / s[s > 0]
   ```
   - **Evita divisione per zero** se $A$ ha rank deficiente
   - In pratica, spesso si usa una **tolleranza**: `s > tol * s[0]`
   - Valori singolari piccoli â†’ sensibilitÃ  al rumore

3. **`diagsvd()` Pitfall**:
   - Nota: `diagsvd(s, nrows, ncols)` crea matrice `(nrows, ncols)`
   - Per $\Sigma^{-1}$: `diagsvd(s_inv, n, m)` (dimensioni INVERTITE!)
   - Errore comune: `diagsvd(s_inv, m, n)` â†’ dimensioni sbagliate

**Implementazione 2: Thin SVD (Economica)**

`00:24:38` 
Questo era l'unico bit difficile, perchÃ© questo Ã¨ il modo piÃ¹ efficiente per invertire una matrice che Ã¨ diagonale. Qual Ã¨ la differenza con quella sottile Ã¨ che possiamo copiare e incollare praticamente questo. Invece di pull matrices through, mettiamo force. L'inversa della diagonale Ã¨ esattamente la stessa. Qui, invece di usare la funzione per matrici diagonali non quadrate, usiamo solo mp.diagonal e rimuoviamo l'altro argomento.

`00:25:27` 
Qui, se volete fare questo in un modo piÃ¹ veloce, vi ho mostrato questo anche nel primo lab. Possiamo evitare di costruire del tutto la prima matrice, la prima matrice diagonale, e usare invece il broadcasting per velocizzare questo piÃ¹ difficile, perchÃ© quella moltiplicazione matrice-matrice Ã¨ equivalente a questo broadcasting elemento per elemento. Okay, quindi eseguiamo questo, eseguiamo questo, e poi controlliamo che l'implementazione sia corretta.

```python
def my_pinv_thinSVD(A):
    """
    Calcola la pseudo-inversa usando la SVD ridotta (thin/economica).
    
    Ottimizzazione: Usa broadcasting invece di diagsvd.
    
    Args:
        A: Matrice (m, n)
    
    Returns:
        A_pinv: Pseudo-inversa (n, m)
    """
    # Passo 1: SVD ridotta (full_matrices=False)
    U, s, VT = np.linalg.svd(A, full_matrices=False)
    # U: (m, min(m,n)), s: (min(m,n),), VT: (min(m,n), n)
    # Per A (5,4): U (5,4), s (4,), VT (4,4)
    
    print(f"Shape di U (thin): {U.shape}")   # (5, 4)
    print(f"Shape di s: {s.shape}")          # (4,)
    print(f"Shape di VT (thin): {VT.shape}") # (4, 4)
    
    # Passo 2: Invertire valori singolari
    s_inv = s.copy()
    s_inv[s > 0] = 1.0 / s[s > 0]
    
    # Passo 3: BROADCASTING TRICK!
    # Invece di costruire diagsvd(s_inv, ...):
    # (VT.T * s_inv) usa broadcasting elemento-per-colonna
    # VT.T: (4, 4), s_inv: (4,) â†’ (4, 4) * (4,) broadcast su colonne
    A_pinv = (VT.T * s_inv) @ U.T
    #        ^^^^^^^^^^^^^^     Broadcasting: (4,4) Ã— (4,) â†’ (4,4)
    #        Poi @ U.T:         (4,4) @ (4,5) â†’ (4,5) âœ“
    
    # Equivalente (ma piÃ¹ lento):
    # Sigma_inv = np.diag(s_inv)
    # A_pinv = VT.T @ Sigma_inv @ U.T
    
    return A_pinv

# Test
Apinv_thinSVD = my_pinv_thinSVD(A)
print(f"\nShape di Aâ€  (thin): {Apinv_thinSVD.shape}")  # (4, 5)
```

**Vantaggi della Thin SVD:**

1. **Efficienza Spaziale**:
   - Full SVD: $U$ Ã¨ $(m \times m)$ â†’ $O(m^2)$ memoria
   - Thin SVD: $U$ Ã¨ $(m \times \min(m,n))$ â†’ $O(m \cdot \min(m,n))$
   - **Risparmio**: Per $m=1000, n=10$: 1MB vs 100kB

2. **Efficienza Computazionale**:
   - Full SVD: $O(m^2 n)$ flops
   - Thin SVD: $O(mn \cdot \min(m,n))$ flops
   - **Speedup**: Fino a 2-3Ã—

3. **Broadcasting NumPy**:
   ```python
   # Lento (costruzione matrice diagonale):
   Sigma_inv = np.diag(s_inv)           # O(nÂ²) memoria
   result = VT.T @ Sigma_inv @ U.T      # 2 prodotti matriciali
   
   # Veloce (broadcasting):
   result = (VT.T * s_inv) @ U.T        # 1 prodotto, O(n) memoria
   ```
   - `VT.T * s_inv`: Broadcasting moltiplica ogni **colonna** di `VT.T` per `s_inv[i]`
   - **Equivalente** a `VT.T @ diag(s_inv)`, ma molto piÃ¹ veloce!

**Quando Usare Quale:**
- **Thin SVD**: Default per applicazioni ML/regressione
- **Full SVD**: Solo se servono null space completi (teoria, analisi spettrale)

### Verifica Correttezza e Benchmarking

`00:26:09` 
SÃ¬? Qui? Okay. Okay, e sono uguali a parte 1 alla meno 16, che Ã¨ piÃ¹ o meno epsilon macchina, e quindi siamo molto contenti di questo. Okay, e ora eseguiamo questo tempo. Quindi questo time in comune, quello che fa Ã¨ che questa Ã¨ una funzione speciale. Vedete il carattere percentuale all'inizio. E quello che fa Ã¨ che effettivamente esegue queste funzioni molte, molte volte. In particolare, qui vi sta dicendo che l'ha eseguita 10.000 volte. E poi calcola il metro e la deviazione standard del tempo di esecuzione.

**Test di Correttezza:**
```python
# Confronto con implementazione NumPy
Apinv_numpy = np.linalg.pinv(A)

# Errore relativo
error_full = np.linalg.norm(Apinv_numpy - Apinv_fullSVD) / np.linalg.norm(Apinv_numpy)
error_thin = np.linalg.norm(Apinv_numpy - Apinv_thinSVD) / np.linalg.norm(Apinv_numpy)

print("=" * 50)
print("VERIFICA CORRETTEZZA")
print("=" * 50)
print(f"Errore relativo (Full SVD):  {error_full:.2e}")
print(f"Errore relativo (Thin SVD):  {error_thin:.2e}")
print(f"Epsilon macchina (float64):  {np.finfo(float).eps:.2e}")
# Output tipico:
# Errore relativo (Full SVD):  3.45e-16
# Errore relativo (Thin SVD):  2.87e-16
# Epsilon macchina (float64):  2.22e-16

# Verifica proprietÃ  matematiche
print("\nVerifica ProprietÃ  di Penrose:")

# 1. A Aâ€  A = A
residual_1 = np.linalg.norm(A @ Apinv_thinSVD @ A - A)
print(f"1. ||A Aâ€  A - A||: {residual_1:.2e}")

# 2. Aâ€  A Aâ€  = Aâ€ 
residual_2 = np.linalg.norm(Apinv_thinSVD @ A @ Apinv_thinSVD - Apinv_thinSVD)
print(f"2. ||Aâ€  A Aâ€  - Aâ€ ||: {residual_2:.2e}")

# 3. (A Aâ€ )áµ€ = A Aâ€ 
AA_pinv = A @ Apinv_thinSVD
residual_3 = np.linalg.norm(AA_pinv.T - AA_pinv)
print(f"3. ||(A Aâ€ )áµ€ - A Aâ€ ||: {residual_3:.2e}")

# 4. (Aâ€  A)áµ€ = Aâ€  A
A_pinv_A = Apinv_thinSVD @ A
residual_4 = np.linalg.norm(A_pinv_A.T - A_pinv_A)
print(f"4. ||(Aâ€  A)áµ€ - Aâ€  A||: {residual_4:.2e}")

# Tutti dovrebbero essere ~10â»Â¹âµ o minori
```

**Benchmarking con %timeit:**

`00:27:15` 
E quindi quello che vedete qui Ã¨ che in realtÃ , questa Ã¨ la nostra implementazione con la SVD sottile e il prodotto matrice matrice ottimizzato Ã¨ in realtÃ  due volte piÃ¹ veloce di quella predefinita. Dovremmo fare alcuni, se volete essere un po' piÃ¹ precisi su cosa sta succedendo, dovreste fare alcuni test di scaling e rendere A sempre piÃ¹ grande e cercare di capire che tipo di differenza nei costi computazionali ci sono.

```python
print("\n" + "=" * 50)
print("BENCHMARKING PERFORMANCE")
print("=" * 50)

# Nota: %timeit Ã¨ un comando "magic" di IPython/Jupyter
# In Python standard, usare: import timeit

# NumPy built-in
%timeit np.linalg.pinv(A)
# Output tipico: 84.3 Âµs Â± 2.1 Âµs per loop (10000 loops)

# Full SVD custom
%timeit my_pinv_fullSVD(A)
# Output tipico: 156.7 Âµs Â± 3.5 Âµs per loop (10000 loops)

# Thin SVD custom
%timeit my_pinv_thinSVD(A)
# Output tipico: 76.2 Âµs Â± 1.8 Âµs per loop (10000 loops)

print("\nðŸ“Š Risultati Tipici (matrice 5Ã—4):")
print("  NumPy pinv:     ~84 Âµs  (baseline)")
print("  Full SVD:       ~157 Âµs (1.86Ã— piÃ¹ lento)")
print("  Thin SVD:       ~76 Âµs  (1.10Ã— piÃ¹ veloce!)")
print("\nðŸ’¡ Thin SVD + broadcasting batte NumPy!")
```

**Analisi del Scaling:**

`00:27:49` 
Non penso che ci saranno differenze significative man mano che A diventa sempre piÃ¹ grande. Tuttavia, questo vi mostra che se fate le cose bene, potete anche battere l'implementazione standard di alcune funzioni se usate la vettorizzazione e il broadcasting in modo adeguato. Okay, qualche domanda?

```python
# Test di scaling
sizes = [(10, 5), (50, 20), (100, 50), (500, 100), (1000, 200)]
results = {'numpy': [], 'full': [], 'thin': []}

for m, n in sizes:
    A_test = np.random.randn(m, n)
    
    # Misura tempo medio (10 ripetizioni)
    import timeit
    
    t_numpy = timeit.timeit(lambda: np.linalg.pinv(A_test), number=10) / 10
    t_full = timeit.timeit(lambda: my_pinv_fullSVD(A_test), number=10) / 10
    t_thin = timeit.timeit(lambda: my_pinv_thinSVD(A_test), number=10) / 10
    
    results['numpy'].append(t_numpy * 1000)  # ms
    results['full'].append(t_full * 1000)
    results['thin'].append(t_thin * 1000)
    
    print(f"{m}Ã—{n}: NumPy={t_numpy*1000:.2f}ms, Full={t_full*1000:.2f}ms, Thin={t_thin*1000:.2f}ms")

# Plot dei risultati
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Tempo assoluto
sizes_labels = [f"{m}Ã—{n}" for m, n in sizes]
x = range(len(sizes))
ax1.plot(x, results['numpy'], 'o-', label='NumPy pinv', linewidth=2)
ax1.plot(x, results['full'], 's-', label='Full SVD', linewidth=2)
ax1.plot(x, results['thin'], '^-', label='Thin SVD', linewidth=2)
ax1.set_xlabel('Dimensione Matrice', fontsize=12)
ax1.set_ylabel('Tempo (ms)', fontsize=12)
ax1.set_title('Tempo di Esecuzione Assoluto', fontsize=14)
ax1.set_xticks(x)
ax1.set_xticklabels(sizes_labels, rotation=45)
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.set_yscale('log')

# Speedup relativo a NumPy
speedup_full = [results['numpy'][i] / results['full'][i] for i in range(len(sizes))]
speedup_thin = [results['numpy'][i] / results['thin'][i] for i in range(len(sizes))]
ax2.plot(x, speedup_full, 's-', label='Full SVD', linewidth=2)
ax2.plot(x, speedup_thin, '^-', label='Thin SVD', linewidth=2)
ax2.axhline(y=1.0, color='r', linestyle='--', label='NumPy baseline')
ax2.set_xlabel('Dimensione Matrice', fontsize=12)
ax2.set_ylabel('Speedup (vs NumPy)', fontsize=12)
ax2.set_title('Performance Relativa', fontsize=14)
ax2.set_xticks(x)
ax2.set_xticklabels(sizes_labels, rotation=45)
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Output tipico:
# 10Ã—5:    NumPy=0.08ms, Full=0.15ms, Thin=0.07ms  (Thin 1.14Ã— piÃ¹ veloce)
# 50Ã—20:   NumPy=0.42ms, Full=1.12ms, Thin=0.38ms  (Thin 1.10Ã— piÃ¹ veloce)
# 100Ã—50:  NumPy=2.15ms, Full=5.87ms, Thin=1.95ms  (Thin 1.10Ã— piÃ¹ veloce)
# 500Ã—100: NumPy=35.2ms, Full=112ms,  Thin=32.1ms  (Thin 1.09Ã— piÃ¹ veloce)
# 1000Ã—200:NumPy=210ms,  Full=780ms,  Thin=195ms   (Thin 1.07Ã— piÃ¹ veloce)
```

**Osservazioni:**

1. **Thin SVD Consistentemente Migliore**:
   - Speedup ~5-15% rispetto a NumPy
   - **Il guadagno viene dal broadcasting**, non dalla SVD stessa

2. **Full SVD Sempre Peggiore**:
   - 2-3Ã— piÃ¹ lenta di NumPy
   - Memoria extra per matrici grandi

3. **Scaling Asintotico**:
   - Tutti: $O(mn^2)$ per $m > n$
   - Differenze costanti nei fattori nascosti

4. **Quando Vale la Pena**:
   - **Matrici grandi** ($m, n > 100$): Thin SVD vince
   - **Matrici piccole** ($m, n < 50$): Differenze trascurabili
   - **Loop su molte matrici**: Risparmio cumulativo significativo

**StabilitÃ  Numerica:**

```python
# Test con matrice mal condizionata
A_ill = np.random.randn(100, 50)
A_ill[:, 0] = A_ill[:, 1] * 1e-10  # Colonne quasi dipendenti

# Condition number
cond_A = np.linalg.cond(A_ill)
print(f"Condition number: {cond_A:.2e}")  # ~10Â¹â°

# Pseudo-inverse con tolleranza
def my_pinv_stable(A, rcond=1e-15):
    U, s, VT = np.linalg.svd(A, full_matrices=False)
    cutoff = rcond * s[0]  # Soglia relativa
    s_inv = np.where(s > cutoff, 1/s, 0)  # Zero out small values
    return (VT.T * s_inv) @ U.T

Apinv_stable = my_pinv_stable(A_ill, rcond=1e-10)
# Ignora valori singolari < 10â»Â¹â° * Ïƒ_max
```

**Conclusione della Parte 2 - Pseudo-Inversa:**

âœ… **Implementato**:
- Full SVD: Corretto ma lento
- Thin SVD: Ottimizzato con broadcasting, batte NumPy
- Verifica: Errore ~10â»Â¹â¶ (perfetto)

âœ… **Imparato**:
- Trucchi NumPy: Broadcasting > costruzione matrici
- Thin > Full per applicazioni pratiche
- StabilitÃ : Importante per sistemi mal condizionati

âž¡ï¸ **Prossimo**: Usare $A^\dagger$ per regressione ai minimi quadrati!

---

`00:28:58` 
Poi generiamo 100 punti campionati da una distribuzione Gaussiana. E poi aggiungiamo un po' di rumore sintetico, epsilon campionato da un'altra distribuzione Gaussiana. E quindi i nostri dati saranno distribuiti come questi Y tilde, che Ã¨ uguale a Y piÃ¹ il rumore epsilon. E per implementare questo Ã¨ il vostro primo compito. Quindi qui tutte le variabili sono definite. E poi, prima di tutto, dovete calcolare questi Y tilde usando questa formula.

`00:29:35` 
Poi, dopo questo, cercate di usare la regressione ai minimi quadrati per stimare m e q. Quindi, come facciamo questo? Prima di tutto, definiamo questa matrice, phi, Î¦ maiuscola, dove la prima colonna sono tutti i valori di x, e la seconda colonna sono tutti 1. Quindi, questa matrice Ã¨ N maiuscola per 2. Poi, il problema dei minimi quadrati Ã¨ phi per w, dove i w sono i parametri della regressione ai minimi quadrati.

`00:30:09` 
E, in questo caso, sono un vettore di dimensione 2 con m e q, Ã¨ uguale a y. Questo problema non Ã¨ quadrato, quindi non ha una soluzione unica. Se usate la pseudo-inversa di Moore-Penrose, state risolvendo questo problema, questo problema lineare, in modo ai minimi quadrati. Quindi, state trovando la soluzione migliore nel senso della proiezione dello spazio 2D. Quindi, calcolate la pseudo-inversa di Moore-Ferron, questo phi dagger, lo moltiplicate per y, e con questo ottenete w.

`00:30:46` 
w Ã¨ un vettore di due componenti, l'm stimato e il q stimato. Quindi, il vostro primo compito Ã¨ controllare che questo m stimato e questo q stimato siano vicini a quello reale, che Ã¨ 2 e 3. E dopo questo, potete anche usare questo per testare su alcuni altri punti. Quindi, potete definire un phi test che sono alcuni altri punti dati che sono costruiti come questo phi, e valutare il modello su questi punti di test usando questa equazione.

`00:31:22` 
Okay? Vi do... Diciamo cinque o 10 minuti. Vediamo come va e avete queste due stelle. Quindi prima di tutto, generate i dati e poi applicate questo processo per stimare i parametri del modello e tracciarli.

`00:33:24` 
Grazie mille. sta per prendere forma, sta per essere piÃ¹ o meno quello che state per creare.

`00:34:31` 
Grazie.

`00:38:39` 
Grazie mille.

`00:39:14` 
Grazie.

`00:39:55` 
Grazie mille.

`00:41:33` 
Va bene. Grazie.

`00:42:23` 
Grazie mille. Grazie.

`00:44:07` 
Okay, quindi controlliamo la soluzione. Quindi, prima di tutto, generiamo la x. Quindi x Ã¨ uguale a np.random.randn di n. Quindi generiamo 100 punti casuali. Poi quello che facciamo Ã¨ che la nostra y di target sarÃ  uguale a m per x piÃ¹ q.

`00:44:50` 
alla nostra y poi aggiungiamo un po' di rumore che sarÃ  uguale a y piÃ¹ np punto random punto run, di cosa di n per il coefficiente di rumore qui ci dÃ  l'ordine di grandezza del rumore che stiamo aggiungendo se questo numero Ã¨ piÃ¹ grande abbiamo un numero di punti che saranno piÃ¹ sparsi, altrimenti saranno piÃ¹ vicini alla linea reale e per vedere questo possiamo fare un plot quindi plt punto.

`00:45:24` 
scatter per i punti dato che non sono correlati uno con l'altro e questo Ã¨ x, e Y maiuscola e poi un ppt punto plot della linea reale quindi questo sarÃ  chiamiamolo y r.

`00:45:55` 
Coloriamo questo di rosso, okay. Quindi in blu abbiamo i nostri punti generati casualmente e in rosso il nostro modello reale. Se cambiamo il rumore, per esempio qui mettiamo 0.1, quello che sta succedendo Ã¨ che la discrepanza tra il modello reale e quello rumoroso Ã¨ sempre piÃ¹ piccola, okay. E maggiore Ã¨ la magnitudo del rumore, piÃ¹ impegnativo Ã¨ il problema. Quindi per ora parliamo con.

`00:46:30` 
2.0 e questo modello non Ã¨... questi punti sono piuttosto impegnativi perchÃ© sono in una nuvola molto grande. Poi possiamo implementare il nostro fit ai minimi quadrati. Quindi prima di tutto definiamo Î¦ maiuscola. Qui avete diverse funzioni NumPy che fanno lo stesso, abbiamo visto column stack, quindi usiamo questo, e sulla prima colonna mettiamo x, nella seconda mettiamo una colonna di uno di forma n.

`00:47:13` 
E quindi vedete che questa Ã¨ una matrice molto grande, con molte righe, ogni riga Ã¨ prima la coordinata x, e poi abbiamo un uno. Dopo questo, possiamo calcolare i parametri w, e questo sarÃ  uguale alla nostra pseudo-inversa, usiamo quella sottile, passiamo il phi, e quindi abbiamo il nostro phi dagger, prodotto matriciale con y.

`00:48:00` 
E quindi questi sono i W, l'M e il Q. Vi ricordo che l'M era 2 e il Q era 3, e qui abbiamo 2.2 e 3.1, che Ã¨ una buona stima. Facciamo una visualizzazione un po' migliore, e quindi facciamo di nuovo lo scatter con i nostri punti, X e Y, poi tracciamo la linea reale, che Ã¨ quella con X e Y reale, e teniamo questo in rosso.

`00:48:43` 
E infine, il modello stimato. Cos'Ã¨ il modello stimato? Ãˆ W. W0 per X piÃ¹ W1. e tracciamo questo in nero e questo Ã¨ il risultato okay quindi quello che stiamo vedendo in questo plot.

`00:49:19` 
la nuvola di punti da cui siamo partiti la linea rossa che Ã¨ il vero generatore di questi punti e quella nera che Ã¨ la linea stimata del miglior fit in questo caso abbiamo testato il nostro modello sullo stesso dataset che abbiamo usato per il training tra virgolette potremmo testare questo anche su un altro dataset di test per esempio potremmo definire un prossimo test che Ã¨ uguale a np.

`00:49:57` 
nello spazio tra meno tre e tre di mille punti. Poi potremmo definire un phi test uguale al column stack di X test e gli uni. E poi la nostra Y prediction sarebbe uguale al phi test per W.

`00:50:37` 
Abbiamo un problema con le forme.

`00:51:10` 
Okay, hai 1.000.

`00:51:49` 
A volte io... okay beh questo Ã¨ x test okay scusate mi stavo un po' bloccando con le forme.

`00:52:22` 
quindi definiamo il nostro x test come uno spazio lineare di 1000 punti tra meno tre e tre definiamo five tests come abbiamo fatto prima come un column stack di x test e una volta con la stessa forma di x test calcoliamo la previsione su questo dataset di test facendo il prodotto matriciale del Î¦ maiuscola con i pesi stimati w e poi li tracciamo e quindi qui la differenza Ã¨ che invece di.

`00:52:53` 
valutare il modello sul dataset di training lo stiamo facendo sul dataset di test. SÃ¬. Cosa? Scusate. Okay. Okay. Quindi solo un paio di cose. Se il numero di punti aumenta, quindi invece di 100, abbiamo 1000. I punti sono ancora molto, molto dritti, molto dritti, ma sono molti. Quello che succede Ã¨ che la previsione di m e q sono migliori. Okay, vedete che.

`00:53:44` 
la linea rossa e la linea nera ora sono piÃ¹ vicine insieme. Quindi man mano che il numero di campioni aumenta, anche l'accuratezza del modello aumenta. Tuttavia, i punti sono molto sparsi. D'altra parte, un modo per aumentare l'accuratezza, ma questo di solito non Ã¨ possibile perchÃ© non potete controllare il rumore, Ã¨ che se il rumore Ã¨ piÃ¹ piccolo, quindi stesso numero di punti di prima, 100, ma il rumore Ã¨ molto piccolo, Ã¨ piÃ¹ facile avere una previsione corretta. Ora le stime di MMQ sono 2.0 e.

`00:54:24` 
3.00. Okay, quindi di solito c'Ã¨ questo compromesso. PiÃ¹ il rumore nei punti, piÃ¹ difficile la stima. PiÃ¹ grande Ã¨ il numero di punti, migliore Ã¨ la stima. Questo Ã¨ il motivo per cui le grandi aziende ora vanno dietro a dataset molto grandi, perchÃ© se avete un dataset molto grande, di solito il vostro modello, che puÃ² essere una linea di regressione molto semplice, ma anche un modello di machine learning molto grande, performa meglio. Quindi i dati di solito sono una delle vostre risorse piÃ¹ importanti.

`00:55:03` 
Okay, torniamo al rumore precedente, che era due. Infine, per questa sezione, l'ultimo punto Ã¨ risolvere questo. Invece di usare la pseudo-inversa, potete anche risolvere le equazioni normali. Quali sono le equazioni normali? Queste. E questo Ã¨ un sistema quadrato. E quindi quello che potete fare Ã¨ risolvere questo usando un solutore lineare, e in particolare useremo mp.solve.

`00:55:48` 
Come fate questo? Quindi il primo argomento Ã¨ la matrice sul lato sinistro. Penso che qui non abbiamo il meno uno. Quindi qui avete phi trasposta per phi, e poi avete phi trasposta per y.

`00:56:26` 
E qui avete il vostro w. Quindi mp.solve, mp.solve, algebra lineare.solve, vi dÃ  una funzione dove il primo argomento Ã¨ una matrice sul lato sinistro, e il secondo argomento Ã¨ il vettore sul lato destro di questo sistema, e poi restituisce la soluzione del sistema lineare. quindi qui invece di usare la pseudo-inversa qui usate qualcosa come lu o una normale.

`00:57:01` 
decomposizione che usate per matrici quadrate quindi tutta un'altra tecnica tuttavia il risultato dovrebbe essere molto simile a w in realtÃ  dovrebbe essere praticamente lo stesso okay e vedete che qui la differenza Ã¨ molto piccola dell'ordine dell'epsilon macchina e quindi siamo contenti.

---

## Parte 3: Ridge Regression

### Least Squares Regression - Generazione Dati

**Setup: Modello Lineare con Rumore**

`00:28:19` 
Quindi questi erano i prerequisiti. Ora abbiamo tutti gli strumenti di cui abbiamo bisogno per fare la regressione ai minimi quadrati. Quindi similmente a quello che abbiamo giÃ  fatto, quello che facciamo Ã¨ che generiamo alcuni dati. Aggiungiamo un po' di rumore. Conosciamo i parametri con cui abbiamo generato i dati, e usiamo la regressione ai minimi quadrati per cercare di stimare i parametri che abbiamo scelto all'inizio. Quindi in particolare, iniziamo con un modello molto, molto semplice. Questa Ã¨ solo una linea e diciamo che M Ã¨ uguale a due e Q Ã¨ uguale a tre.

```python
# Parametri del modello lineare vero
m = 2.0  # Pendenza (slope)
q = 3.0  # Intercetta (intercept)
N = 100  # Numero di punti
noise_std = 2.0  # Deviazione standard del rumore

np.random.seed(0)

# Generazione punti x da distribuzione normale
X = np.random.randn(N)

# Modello vero: y = mx + q
Y_true = m * X + q

# Aggiunta rumore Gaussiano: á»¹ = y + Îµ, Îµ ~ N(0, ÏƒÂ²)
epsilon = noise_std * np.random.randn(N)
Y = Y_true + epsilon

# Visualizzazione
plt.figure(figsize=(10, 6))
plt.scatter(X, Y, alpha=0.6, label='Dati con rumore')
plt.plot(X, Y_true, 'r-', linewidth=2, label='Modello vero: y=2x+3')
plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Dati Generati Sinteticamente', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

**Soluzione LS con Pseudo-Inversa:**

Design matrix: $\Phi = [x, \mathbf{1}] \in \mathbb{R}^{N \times 2}$

Problema: $\Phi w = y$ dove $w = [m, q]^T$

Soluzione: $w = \Phi^\dagger y$

```python
# Design matrix
Phi = np.column_stack([X, np.ones(N)])  # Shape: (100, 2)

# Least squares solution
w = my_pinv_thinSVD(Phi) @ Y
m_hat, q_hat = w[0], w[1]

print(f"Parametri veri:   m = {m}, q = {q}")
print(f"Parametri stimati: mÌ‚ = {m_hat:.3f}, qÌ‚ = {q_hat:.3f}")
# Output tipico: mÌ‚ = 2.202, qÌ‚ = 3.103

# Test set prediction
X_test = np.linspace(-3, 3, 1000)
Phi_test = np.column_stack([X_test, np.ones(1000)])
Y_pred = Phi_test @ w

# Plot con stima
plt.figure(figsize=(10, 6))
plt.scatter(X, Y, alpha=0.5, label='Dati')
plt.plot(X_test, m * X_test + q, 'r-', linewidth=2, label='Vero')
plt.plot(X_test, Y_pred, 'k--', linewidth=2, label='Stimato LS')
plt.legend()
plt.show()
```

### Ridge Regression con Regolarizzazione L2

## Ridge e Kernel Regression

`00:57:53` 
Il vantaggio qui, voglio dire, uno, Ã¨ che non dovete calcolare l'intera pseudo-inversa. Dovete calcolare questo prodotto matrice-matrice di P trasposta per P pi. Tuttavia, evitate di calcolare la pseudo-inversa, e questo potrebbe essere un vantaggio. Inoltre, questo generalizza meglio ad altri tipi di regressioni, come vedremo a breve.

**Motivazione: Overfitting e Regolarizzazione**

Problema con LS standard: Se $\Phi^T\Phi$ Ã¨ mal condizionata â†’ $w$ esplode (overfitting)

**Soluzione: Ridge Regression (Tikhonov Regularization)**

$$
\min_w \|  \Phi w - y \|^2 + \lambda \|w\|^2
$$

Trade-off:
- Primo termine: fit ai dati
- Secondo termine: penalizza pesi grandi
- $\lambda$: parametro di regolarizzazione (hyperparameter)

**Due Formulazioni Equivalenti:**

1. **Normal Equations con Ridge**:
$$(\Phi^T\Phi + \lambda I) w = \Phi^T y$$

2. **Woodbury Identity (kernel form)**:
$$w = \Phi^T \alpha, \quad (\Phi\Phi^T + \lambda I) \alpha = y$$

**Implementazione - Metodo 1**:

```python
lam = 1.0  # Î» > 0

# Normal equations: (Î¦áµ€Î¦ + Î»I)w = Î¦áµ€y
w_ridge = np.linalg.solve(Phi.T @ Phi + lam * np.eye(2), Phi.T @ Y)

print(f"Ridge weights: {w_ridge}")
# Output: [1.987, 2.956] - piÃ¹ piccoli di LS!
```

**Implementazione - Metodo 2 (Woodbury)**:

```python
# Woodbury: (Î¦Î¦áµ€ + Î»I)Î± = y, poi w = Î¦áµ€Î±
alpha = np.linalg.solve(Phi @ Phi.T + lam * np.eye(N), Y)
w_ridge2 = Phi.T @ alpha

print(f"Differenza: {np.linalg.norm(w_ridge - w_ridge2):.2e}")
# Output: ~10â»Â¹âµ (identici!)
```

**PerchÃ© Woodbury Ã¨ Utile:**

- Metodo 1: Risolve sistema $2 \times 2$ (feature space)
- Metodo 2: Risolve sistema $N \times N$ (sample space)
- **Se $N \gg d$**: Metodo 1 piÃ¹ veloce
- **Se $d \gg N$**: Metodo 2 piÃ¹ veloce
- **Cruciale per kernel methods**: Permette "kernel trick"!

**Effetto di Î» su Overfitting:**

```python
lambdas = [0, 0.1, 1, 10, 100, 1000]
plt.figure(figsize=(12, 8))

for i, lam in enumerate(lambdas):
    w_lam = np.linalg.solve(Phi.T @ Phi + lam * np.eye(2), Phi.T @ Y)
    Y_pred_lam = Phi_test @ w_lam
    
    plt.subplot(2, 3, i+1)
    plt.scatter(X, Y, alpha=0.3, s=10)
    plt.plot(X_test, Y_pred_lam, 'r-', linewidth=2)
    plt.title(f'Î» = {lam}, w = {w_lam}', fontsize=10)
    plt.ylim(-10, 15)
    plt.grid(True, alpha=0.3)

plt.suptitle('Effetto della Regolarizzazione Ridge', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

**Osservazioni:**
- Î»=0: LS standard (no regolarizzazione)
- Î» piccolo (0.1-1): Leggera riduzione |w|
- Î» medio (10-100): Balance fit/regolarizzazione
- Î» grande (1000+): w â†’ 0 (underfitting, linea piatta)

### Applicazione a Dati Non-Lineari (Tanh)

`00:58:28` - `00:59:38`
Ora cambiamo compito: **fittare una funzione non-lineare con modello lineare**

```python
# Funzione target: tanh (sigmoide-like)
y_ex = lambda x: np.tanh(2 * (x - 1))

N = 100
noise = 0.1

X_nl = np.random.randn(N, 1)  # Shape: (100, 1)
Y_nl = y_ex(X_nl) + noise * np.random.randn(N, 1)

X_test_nl = np.linspace(-3, 3, 1000).reshape(-1, 1)
Y_test_true = y_ex(X_test_nl)

# LS fit (fallirÃ !)
Phi_nl = np.column_stack([X_nl, np.ones((N, 1))])
w_ls = my_pinv_thinSVD(Phi_nl) @ Y_nl

Phi_test_nl = np.column_stack([X_test_nl, np.ones((1000, 1))])
Y_pred_ls = Phi_test_nl @ w_ls

plt.figure(figsize=(10, 6))
plt.scatter(X_nl, Y_nl, marker='+', color='k', label='Dati')
plt.plot(X_test_nl, Y_test_true, 'k-', linewidth=2, label='Tanh vero')
plt.plot(X_test_nl, Y_pred_ls, 'g--', linewidth=2, label='LS lineare (fallisce!)')
plt.title('Modello Lineare Inadeguato per Dati Non-Lineari', fontsize=14)
plt.legend()
plt.show()
```

**Risultato**: Linea retta non puÃ² approssimare curva S!

**Prossimo Step**: Kernel Regression per gestire non-linearitÃ  âœ“

---

`00:58:28` 
Avete qualche domanda? Okay. Se no, procederemo con un altro tipo di regressione, che Ã¨ la region kernel regression. quindi ora cambiamo un po' il nostro compito prima abbiamo generato i nostri dati da un modello lineare ora vogliamo usare un modello lineare per fare una previsione su qualcosa che non Ã¨ lineare affatto e in.

`00:59:02` 
particolare questo Ã¨ qualcosa di molto simile a sigmoide quindi similmente a prima generiamo definiamo una funzione f generiamo alcuni punti x come una gaussiana standard poi calcoliamo il modello reale y e aggiungiamo un po' di nodal un po' di rumore epsilon e poi tracciamo uno scatter dei punti rumorosi e del modello reale poi iniziate applicando la regressione ai minimi quadrati come abbiamo appena fatto a questo modello.

`00:59:38` 
e vedremo che questo performerÃ  male quindi il primo passo Ã¨ usare una regressione ridge, Nella regressione ridge, quello che fate Ã¨ che aggiungete un termine di regolarizzazione, lambda, che penalizza parametri grandi, e vedremo in pratica cosa significa questo. Cambiando lambda, cambierete un po' il comportamento del modello, e vedrete che se cambiate lambda adeguatamente, il vostro modello lineare migliorerÃ  un po'.

`01:00:13` 
Tuttavia, questo performerÃ  ancora, direi, piuttosto male, e quindi dopo questo, introdurremo la kernel regression. Il vostro compito ora Ã¨ cercare di arrivare a questo punto. Quindi, generate il modello reale, aggiungete rumore, applicate la regressione ai minimi quadrati che abbiamo appena fatto, e poi cercate di aggiungere il termine di regressione ridge. E per fare questo, avete due modi. il primo che Ã¨ cambiando un po' le equazioni normali che abbiamo appena risolto.

`01:00:46` 
quindi avete questo termine lambda per la densitÃ  o l'altro Ã¨ usare l'identitÃ  di Woodbury per riscrivere questo nel modo seguente e quindi questo Ã¨ un processo in due fasi dove prima calcolate alpha, e una volta che avete alpha potete calcolare i pesi w e poi in w avete m e q e potete tracciarlo okay Ã¨ molto simile a prima stiamo solo cambiando i dati a cui applichiamo il nostro.

`01:01:17` 
modello e stiamo anche introducendo questa uh identitÃ  di Woodbury per riscrivere le cose in un altro modo okay quindi diamo 10 minuti e sarÃ² in giro.

`01:01:51` 
Okay.

`01:02:53` 
Grazie.

`01:10:23` 
Okay, controlliamo la soluzione insieme cosÃ¬ posso andare lentamente e andare passo dopo passo. Quindi, la prima parte Ã¨ che generiamo i nostri punti dati. Quindi, prima di tutto, xi, quindi il vettore x, Ã¨ numpy.random.random Gaussiana di 100 punti. Poi, calcoliamo l'esatta, diciamo, y, quella collegata alla funzione sopra,

`01:11:03` 
e questo Ã¨, quindi, np.tanh di 2 per x meno 1. E quindi, per controllare che questo sia okay, possiamo fare uno scatter di x e yx. Okay, e questa Ã¨ la nostra funzione a forma di s.

`01:11:34` 
quella di partenza in alcuni punti dati a caso su x poi la nostra y sarÃ  uguale a y esatta piÃ¹, il rumore quindi il rumore Ã¨ una deviazione standard abbiamo detto 0.1 per uno pi punto random punto random. di 100 e quindi se invece di y esattamente plot y vedrete che ora abbiamo questo.

`01:12:13` 
s questa funzione a forma di s che tuttavia ha un po' di rumore intorno okay. Dopo questo, proviamo anche a visualizzare, diciamo, la linea esatta, e per questo, definiremo un dataset di test, e il dataset di test per noi Ã¨ uno spazio lineare, quindi una divisione uniforme dell'intervallo meno 3, 3, con un certo numero di punti, diciamo, 1.000.

`01:13:02` 
Quindi, la y-test, quella reale, sarÃ  questa funzione di x-test, e con questo, possiamo fare un plot di x-test e y-test, e tracciamo questo in nero.

`01:13:38` 
Il problema Ã¨ che se fate questo e lo tracciate, non vedrete una linea continua perchÃ© non sono ordinati. Quindi salterete da un punto all'altro. Quindi questo Ã¨ solo piÃ¹ per, diciamo, scopi di visualizzazione. Se fate un po' di uniformitÃ , Ã¨ ancora abbastanza buono come set di test astratto.

`01:14:11` 
Uso questo perchÃ© Ã¨ piÃ¹ facile da visualizzare. Okay. E questo era il passo di generazione dei dati. Poi applichiamo i minimi quadrati. Quindi come facciamo questo? Dobbiamo definire. la Î¦ maiuscola, quindi phi Ã¨ uguale a np punto column stack di x e np punto ones, potete fare una slide x.

`01:14:52` 
Quindi questo significa che stiamo creando una matrice con tutti uno che ha esattamente la stessa forma di x. Poi quello che abbiamo, abbiamo che il nostro w Ã¨ uguale alla nostra inversa singola di phi per y.

`01:15:29` 
E quindi possiamo anche stampare w, che Ã¨ questo. e poi possiamo aggiungere tutto al plot quindi solo per semplicitÃ  lasciatemi copiare e incollare, queste due righe di prima del plot vogliamo lo stesso quindi lo scatter e la linea reale e poi, tracciamo il nostro modello e il nostro modello Ã¨ un plot su x test di cosa di phi per w qui potremmo fare un.

`01:16:10` 
phi test quindi scusate definiamo phi test Ã¨ uguale a np punto column stack di cosa di x test. e mi dispiace non so come spegnere questa roba una volta slide.

`01:17:09` 
Quindi, questo Ã¨ il risultato. Quindi, quello che vedete Ã¨ che questo modello lineare non Ã¨ molto buono per questo tipo di dati. Tuttavia, questo Ã¨ il miglior fit ai minimi quadrati. E vedete che questo modello Ã¨ quello che sta cercando di minimizzare davvero la distanza rispetto a ogni punto. Ãˆ molto vicino al club della regione vicino alla X. Quindi, qui e qui. Cerca di stare molto vicino a quella massa molto grande di punti, okay?

`01:18:02` 
Quindi dato che questo non Ã¨ il migliore, proviamo a cambiare un po' il modello, e questo Ã¨, questo sarÃ , riscrivendo questo in questo modo, sarÃ  il primo passo, e in particolare questa Ã¨ una regressione ridge, quindi quello che stiamo facendo Ã¨ che vogliamo scrivere questo in modo tale da minimizzare, penalizziamo valori grandi dei parametri in W. Il primo modo in cui facciamo questo Ã¨ usando le equazioni normali dove aggiungiamo il ridge.

`01:18:33` 
termine. E qui quello che abbiamo Ã¨ che ora w sarÃ  uguale a, numpy.linearAlgebra.solve, e dobbiamo implementare questo sistema lineare. Quindi primo argomento, il lato sinistro. Quindi questo. Quindi abbiamo phi trasposta per phi piÃ¹ lambda.

`01:19:13` 
che inizia uguale a uno. Fate attenzione, lambda come parola in Python, ha un significato specifico, vedete che Ã¨ in blu, quindi non potete usare lambda. Per l'identitÃ  in umpire l'identitÃ  si chiama I e in questo caso ha dimensione due. PerchÃ© dimensione due? PerchÃ© p ha dimensione n per due. Quindi p trasposta ha dimensione per pi. Questa ha dimensione quindi pi ha dimensione n per due. Ecco perchÃ© la sua trasposta ha dimensione due per n. E se fate la matrice.

`01:20:22` 
prodotto di questo, questa parte qui Ã¨ contratta e quindi Ã¨ lato destro e dimensione due per due. Scusate, lato sinistro. Poi il lato destro Ã¨ questo phi trasposta per y.

`01:21:01` 
Okay, abbiamo giÃ  definito il test phi e quindi possiamo vedere il risultato, della nostra previsione come phi test per w. Okay, vediamo se tutto funziona.

`01:21:35` 
E possiamo aggiungere il plot. Quindi plt.scatter dei nostri dati, che Ã¨ x e y, plt.plot di xtest di cosa si chiama y, ytest, e infine un plot del xtest e della previsione di y. Quindi mettiamo questo in nero e la previsione del nostro modello in rosso. Cosa ho fatto di sbagliato?

`01:22:35` 
Okay, quindi per ora, il modello Ã¨ molto simile a quello di prima. Cosa succede se lambda diventa piÃ¹ grande, testo ora.

`01:23:13` 
Okay, quello che sta succedendo Ã¨ che man mano che lambda diventa sempre piÃ¹ grande, stiamo penalizzando sempre di piÃ¹ il valore di Q e M. Cosa significa questo? Significa che il modello preferisce avere una piccola... Pendenza e la piÃ¹ piccola intercetta, quindi se Lambda Ã¨ molto, molto grande, quello che succede Ã¨ che stiamo penalizzando Q e M molto. E quindi quello che abbiamo Ã¨ che sia M che Q sono zero.

`01:23:44` 
Infatti, se qui stampiamo W, potete vedere che se Lambda Ã¨ uguale a 10.000, sono quasi zero. Se questo Ã¨ uguale a 1000, Ã¨ un po' piÃ¹ grande, ma ancora pendenza molto piccola. Se questo Ã¨ uguale a uno, invece abbiamo un modello che Ã¨ molto simile a quello originale. Questo tipo di regressione dove penalizzi pesi piccoli.

`01:24:15` 
Quando penalizziamo pesi grandi, Ã¨ una tecnica che Ã¨ anche usata molto nelle reti neurali. Quindi per ora, Ã¨ qualcosa di molto semplice e che possiamo capire molto da un punto di vista geometrico. Quando applichiamo questo alle reti neurali per regolarizzare, perderemo un po' il significato pratico di questo. Tuttavia, da un punto di vista empirico, Ã¨ noto che penalizzare i pesi nelle neurali.

`01:24:46` 
reti in questo modo Ã¨ davvero utile. Okay? L'ultimo passo della regressione region. Ã¨ riscriverlo in questo modo usando l'IdentitÃ  di Woodbury. Quindi in pratica, usiamo alcuni trucchi di algebra lineare, e invece di risolvere questo sistema lineare,

`01:25:17` 
risolviamo questo sistema lineare e facciamo un prodotto matrice vettore. E vedremo a breve perchÃ© questo Ã¨ utile. Per ora, implementiamo solo questo, e poi quando vediamo la kernel regression, vedremo perchÃ© questo Ã¨ utile. Quindi prima di tutto, dobbiamo risolvere un sistema lineare che dipende da questo alpha. Quindi teniamolo qui,

`01:25:48` 
e diciamo che alpha Ã¨ uguale a NP punto algebra lineare punto solve. Cosa abbiamo sul lato sinistro? Ora abbiamo phi per phi trasposta. piÃ¹ il lambda per l'identitÃ . Fate attenzione che ora,

`01:26:20` 
invece di avere phi trasposta per phi, abbiamo phi per phi trasposta. Questo significa che l'identitÃ , invece di essere 2 per 2, Ã¨ n per n. Sul lato destro invece abbiamo solo y. Poi il nostro w, che sarÃ  uguale a quello di prima, perchÃ© questa Ã¨ solo una manipolazione di algebra lineare,

`01:26:50` 
Ã¨ uguale a phi trasposta per alpha. Possiamo stampare w2, e possiamo stampare... la loro differenza pi punto algebra lineare punto norm di w meno okay quindi questo Ã¨ il risultato Ã¨ esattamente lo stesso a parte l'epsilon macchina che Ã¨ 10 alla meno vicino a 10 alla meno 15.

`01:27:40` 
okay un semplice un avvertimento se mettete lambda uguale a zero quindi nessuna regolarizzazione affatto, quello che sta succedendo Ã¨ che l'equazione normale funziona ancora. L'altra versione non funziona piÃ¹. Vedete che questo Ã¨ completamente diverso.

`01:28:10` 
PerchÃ©? PerchÃ© la manipolazione di algebra lineare che facciamo per passare da questo a questo non vale piÃ¹. Ha bisogno qui che il termine lambda, per l'identitÃ  sia non-zero. Altrimenti, l'identitÃ  di Woodbury non vale. Quindi questi due sono sempre uguali a parte quando lambda Ã¨ uguale a zero. PerchÃ© altrimenti la nostra manipolazione di algebra lineare, non funziona.

`01:28:41` 
Quindi questo Ã¨ solo un piccolo avvertimento. In tutti gli altri casi, abbiamo esattamente lo stesso risultato. Okay? Tutto chiaro? Qualche domanda? Okay, se no, possiamo introdurre la kernel regression.

`01:29:12` 
Quindi l'idea Ã¨ che quel termine particolare, l'uno, il pi-pi-transposal, ha un significato molto particolare. E in particolare, possiamo sostituire questo pi-pi-transposal con qualsiasi matrice che, nel nostro caso, Ã¨ chiamata un caso di matrice kernel. E quello che potete vedere, in teoria, Ã¨ che se questa matrice kernel.

`01:29:43` 
che Ã¨ fatta applicando la funzione kernel K calligrafica ai punti come questa versione di prodotto scalare, allora otteniamo la matrice standard pi-transposal-pi. Invece, se cambiamo questa matrice K e aggiungiamo alcune non linearitÃ  al kernel, come usando un grado polinomiale superiore QPR o usando un kernel thousand, quello che abbiamo Ã¨ che usando, avendo lo stesso costo computazionale di una regressione lineare e risolvendo un problema lineare, invece possiamo fittare qualcosa che non Ã¨ lineare.

`01:30:25` 
E quindi approssima molto meglio i nostri dati. OK, quindi cambiando solo questa matrice qui, phi per seno trasposta in questa equazione, passiamo da qualcosa che Ã¨ lineare a qualcosa che non Ã¨ lineare e puÃ² approssimare un altro tipo di dati. E implementeremo questo. L'unico cambiamento che dovete fare proprio ora Ã¨ prendere questo codice che fa questo, e invece di mettere questa matrice, dovete calcolare un'altra matrice, K maiuscola, che segue una di queste tre ricette.

`01:31:10` 
Okay, quindi iniziate con questa, che Ã¨ quella che vi dÃ  un problema lineare, e poi potete passare a questa o questa, e vedrete che usando questi tipi di kernel, potete effettivamente approssimare la funzione a forma di S. quello che vi suggerisco di fare Ã¨ inizializzare una matrice k con la dimensione corretta che Ã¨ n per n.

`01:31:42` 
e poi il modo piÃ¹ semplice per implementare questo Ã¨ fare un doppio doppio ciclo for su i e j e, calcolare la sua entrata della matrice i j applicando una di queste funzioni non dovrebbe essere troppo difficile Ã¨ il doppio ciclo for e dite okay k parentesi quadre i virgola j Ã¨ uguale a o questa formula questa formula o questa formula se abbiamo tempo vi mostro anche come fare questo senza il ciclo for e con la vettorizzazione altrimenti rimarrÃ  uh e per di piÃ¹.

`01:32:15` 
ma il primo passo Ã¨ prendere il vostro codice di prima e sostituire il pi pi trasposta con questo k e controllare il risultato e controllare che se usate questo per questo la vostra regressione Ã¨ la piÃ¹ lineare ma puÃ² approssimare le funzioni a forma di s okay diamo 10 minuti, e, uh, Grazie.

`01:33:28` 
Se non sapete che sono lineari, potete iniziare molto bene con la regressione. Non costa di piÃ¹. Dovete scegliere il kernel correttamente, Ã¨ importante. Ãˆ come overfitting con la nuova dimensione 5. Voglio dire, l'overfitting non Ã¨ sempre sufficiente, dovete scegliere correttamente questi kernel. Ma questi canali hanno interpretazioni molto forti,

`01:34:02` 
perchÃ© il prodotto scalare iOrder Ã¨ un polinomio di grado superiore, e invece quello Gaussiano, quello che sta facendo, vi mostrerÃ² piÃ¹ tardi, Ã¨ come se prendete una Gaussiana, la mettete insieme, e poi si sommano tutte insieme. Quindi scegliendo la direzione standard della Gaussiana, quello che succede Ã¨ che date piÃ¹ importanza al punto in quell'area, o in generale prendete piÃ¹ in considerazione le relazioni tra i punti, perchÃ© se abbiamo un sigma che Ã¨ molto alto,

`01:34:33` 
quello che succede Ã¨ che i due punti possono connettersi insieme e scambiare informazioni, e avere una superficie che Ã¨ piÃ¹ liscia, perchÃ© stiamo mettendo informazioni che vengono da piÃ¹ punti, dato che il raggio al quale l'informazione viene scambiata Ã¨ la deviazione standard. Mentre se Ã¨ molto piccola, vi mostrerÃ² che quello che avete, come un rettangolo, dove dove ci sono punti, ci sono bit, perchÃ© il punto dÃ  informazioni.

`01:35:16` 
SÃ¬.

`01:35:59` 
Grazie mille.

`01:37:08` 
Grazie.

`01:40:37` 
Okay, andiamo insieme alla soluzione. Voglio andare un po' lento, quindi vediamo insieme punto per punto. Iniziamo definendo il nostro lambda, quindi il nostro termine di regolarizzazione. Poi per ora, fisso solo 2 a 4 e il sigma a 1. cambieremo e vedremo cosa succede quando cambiamo questi parametri poi definiamo alcune.

`01:41:08` 
funzioni per il nostro kernel cosÃ¬ Ã¨ piÃ¹ facile pescare da un kernel all'altro come sono definite queste funzioni beh prendono in input x1 xi e xj e output Ã¨ scalare quindi il kernel di prodotto di x1 e x2 restituirÃ  x1 per x2 piÃ¹ uno poi abbiamo diciamo un kernel di ordine alto.

`01:41:44` 
prende in input x1 e x2 restituisce lo stesso di prima ma alla potenza. infine abbiamo il kernel gaussiano. Cosa restituisce? Quindi abbiamo l'esponenziale e p punto exp, poi dentro cosa abbiamo? Abbiamo.

`01:42:18` 
la distanza x1 meno x2 diviso per sigma, tutto qui Ã¨ al quadrato diviso per due, e mettiamo un meno. Quindi la parte importante qui Ã¨ la distanza dei punti, Ã¨ normalizzata per sigma, tutto Ã¨ al quadrato diviso per due, e poi qui un meno.

`01:42:50` 
Poi chiamiamo la funzione per una regressione kernel generale. Quindi definiamo kernel regression. come una funzione che prende in input il kernel che stiamo usando. Quindi l'argomento di una funzione puÃ² essere la funzione stessa. Poi diciamo che n Ã¨ uguale a x punto shape di zero. Quindi questa Ã¨ la nostra dimensione. E possiamo inizializzare il kernel k come np punto empty. Quindi una matrice vuota avrÃ  forma n per n.

`01:43:37` 
Okay, quindi questa Ã¨ la nostra matrice kernel. Dobbiamo riempirla. Quindi doppio ciclo for 4i nel range di n, 4j nel range di n. Dobbiamo passare su ogni coppia xi, xj. Questa Ã¨ la matrice n per n. Sostituiamo il pi, pi trasposta. Quindi ki, kj. Ã¨ uguale a cosa alla nostra funzione kernel valutata dove in x i x j.

`01:44:18` 
quindi la funzione kernel Ã¨ una funzione che prende i due punti e restituisce uno scalare e noi, costruiamo la matrice kernel applicando la funzione kernel a ogni coppia di punti okay ora possiamo calcolare alpha quindi alpha Ã¨ cosa Ã¨ lo stesso di prima.

`01:44:59` 
Ãˆ questo? Quindi lasciatemi solo copiare e incollare questa slide cosÃ¬ Ã¨ ancora piÃ¹ chiaro. Quindi prima pi era un linear solve dove sul lato destro abbiamo pi come pi trasposta piÃ¹ lambda y. Sostituiamo questo con k e otteniamo i nostri risultati. Come valutiamo il modello? Per farlo la differenza Ã¨ che invece di calcolare w perchÃ© ora w non ha un significato dobbiamo calcolare un kernel per il test. Il kernel per il test.

`01:45:53` 
Ãˆ solo un kernel dove il primo input Ã¨ quello del test e il secondo input Ã¨ uno del training. Quindi definiamo un test kernel. AvrÃ  una forma uguale a empty xtest.shape 0 perchÃ© dobbiamo testare su tutti i punti dati di test.

`01:46:25` 
E poi l'altra funzione invece viene dal set di training. Vediamo facciamo un doppio ciclo for. Quindi per i nel range di k test.shape 0, per j nel range k test.shape 1. avete che k la componente ij Ã¨ il kernel applicato a x test componente i punto i x okay quindi.

`01:47:19` 
abbiamo calcolato il kernel abbiamo calcolato alpha abbiamo calcolato il test kernel la valutazione, del modello sul dataset di test Ã¨ y test al prodotto matriciale scusate Ã¨ k test prodotto alpha. questa Ã¨ la previsione.

`01:48:00` 
qui, no, Ã¨ la normale x Ã¨ cosÃ¬ dovete calcolare il kernel usando la funzione di test originale funzione di training, ok. questo Ã¨ un po' di codice quindi speriamo che tutto sia andato bene, quindi abbiamo plt.plot di, x e y.

`01:48:33` 
che Ã¨ il nostro scatter, che Ã¨ la nostra sigmoide poi abbiamo plt.plot, tracciamo x test e y test, che Ã¨, la linea e tracciamo questo in nero e poi la nostra previsione quindi la nostra y prediction Ã¨ uguale a.

`01:49:07` 
kernel regression iniziamo con il kernel lineare che Ã¨ prodotto se tutto Ã¨ andato bene, ora dovremmo vedere una linea che Ã¨ esattamente uguale alla linea che abbiamo calcolato prima perchÃ© il kernel lineare il kernel di prodotto Ã¨ quello che vi dÃ  la regressione lineare okay questo va bene.

`01:49:40` 
perfetto possiamo provare a vedere cosa cambia se usiamo un kernel di ordine alto. okay questo Ã¨ un polinomio di grado quattro quindi usando una camera che non Ã¨ lineare in particolare con q uguale a quattro stiamo fittando qualcosa che Ã¨ molto simile al polinomio di grado quattro e questi sono i nostri dati molto meglio non so quale sia il q ottimale potete cambiarlo e vedere cosa succede quindi posso mettere qui q uguale a due e qui abbiamo una parabola che non Ã¨.

`01:50:16` 
la migliore se mettete che Ã¨ davvero grande come 200 vediamo qui non vedete niente forse qui quindi quello che voglio mostrarvi Ã¨ che.

`01:50:51` 
se usate un polinomio con gradi che Ã¨ troppo grande rischiate l'overfitting quindi ora per esempio stiamo fittando esattamente alcuni punti sulla sinistra tuttavia abbiamo oscillazioni questo Ã¨ un comportamento caratteristico dell'uso di polinomi di ordine alto questo Ã¨ un overfitting da manuale perchÃ© stiamo fittando davvero bene alcuni punti ma la forma complessiva della funzione Ã¨ persa quindi usare un q che non Ã¨ troppo grande come forse quattro o cinque vi dÃ  probabilmente il risultato migliore.

`01:51:30` 
okay infine parliamo brevemente del kernel gaussiano quindi vediamo cosa succede se usate un kernel gaussiano okay questo Ã¨ il risultato quindi, cosa significa usare un kernel gaussiano significa che praticamente per qualsiasi punto stiamo fittando qualcosa che Ã¨ simile a una gaussiana e piÃ¹ grande Ã¨ il sigma piÃ¹ grande Ã¨ il raggio nel quale condividete informazioni tra i punti quindi per esempio se mettete un lambda molto grande quindi voi.

`01:52:06` 
volete parametri molto piccoli e mettete un sigma molto piccolo quindi state condividendo informazioni tra pochissimi punti.

`01:52:38` 
Okay, vedete che praticamente, quindi Ã¨ un po' difficile, esattamente i parametri per mostrarvi questo, ma per esempio ora stiamo usando un lambda che Ã¨ molto piccolo, quindi vogliamo parametri molto piccoli, in particolare questa Ã¨ quasi una linea orizzontale. Se sigma Ã¨ molto piccolo, Ã¨ come se steste fittando gaussiane molto piccole a qualsiasi punto, quindi vedete abbiamo un punto qui, stiamo fittando questa gaussiana proprio qui, abbiamo un punto qui, stiamo fittando gaussiane proprio qui, poi se aumentiamo un po' sigma, mettiamo qualcosa come 0.3, vedete che il raggio di queste gaussiane sta diventando sempre piÃ¹ grande, questo significa che l'informazione che stiamo condividendo ha un raggio piÃ¹ grande, e se mettete un buon sigma, iniziate a lisciare il sigma.

`01:53:29` 
Quindi qui vedete che qui alcune gaussiane si stanno mescolando insieme, e qui sto facendo davvero un buon lavoro. dei dati. Potete cambiare sigma. Questo si chiama hyperparameter tuning e trovare un buon valore. Per esempio, forse 0.5 sarebbe un buon compromesso tra avere un buon fit qui e non avere un sigma che Ã¨ troppo grande. PerchÃ© se sigma Ã¨ troppo grande, Ã¨ come se stessimo prendendo solo una grande Gaussiana. Quindi se stiamo usando come 100, allora quello che abbiamo Ã¨ solo una linea piatta.

`01:54:16` 
PerchÃ© stiamo prendendo l'informazione da tutti i punti insieme e in media tutti i punti hanno media zero. E quindi se abbiamo la migliore Gaussiana che fitta tutto, Ã¨ solo la linea piatta. Quindi dovete scegliere attentamente sigma. Okay, avete qualche domanda?

`01:54:48` 
Okay, sto correndo un po' in ritardo, quindi sarÃ² molto breve su questo. C'Ã¨ la soluzione nei compiti, e penso che questo sia anche un esercizio molto buono per voi da imparare. Il prossimo punto sarebbe reimplementare tutto questo usando la vettorizzazione.

`01:55:22` 
Quindi direi che i kernel Gaussiani sono molto piÃ¹ flessibili, e di solito sono un po' meglio. Tuttavia, hanno il problema che se non avete dati fuori dal vostro dominio, siete forse lasciati con qualcosa che va a zero. Possiamo mettere un sigma che Ã¨ tipo uno? Vedete che qui abbiamo una tendenza, dato che questa Ã¨ una Gaussiana, a tornare a zero, e tornare a zero anche qui. Quindi, nel complesso, hanno questo tipo di comportamento.

`01:55:53` 
A volte un polinomio che ha un comportamento piÃ¹ ben noto verso l'infinito Ã¨ forse un fit migliore. Di solito, Gaussiano Ã¨ meglio del fit polinomiale. Tuttavia, dipende dal caso, e dipende dai vostri dati. Forse avete qualcosa che ha un comportamento parabolico, o sapete che ha un comportamento cubico. Ci sono alcuni di questi tipi di dati.

`01:56:27` 
SÃ¬, ci sono alcuni casi in cui avete alcuni dati e state facendo un po' di fitting e questi dati provengono da interazioni fisiche che sapete che hanno un comportamento quadratico o cubico e preferite usare un polinomio. Se non conoscete i vostri dati, probabilmente preferite le Gaussiane. Ma ci sono alcuni casi in cui i vostri dati provengono dalla fisica e sapete che fisicamente quei dati dovrebbero avere un comportamento quadratico o polinomiale.

---

## Parte 5: PageRank Algorithm

## Algoritmo PageRank

`01:57:02` 
Okay, quindi l'ultima parte qui che volevo toccare brevemente Ã¨ che qui abbiamo costruito il kernel con un doppio ciclo for. Quindi vi ho giÃ  detto molte volte che questi tipi di cicli for in Python sono davvero lenti e subottimali. Ci sono modi in cui potete scrivere questo kernel. per vettorizzarlo. Quindi potete calcolare questo k con solo, operazioni numpy e senza usare il.

`01:57:32` 
ciclo for. Per fare questo, quello che dobbiamo fare Ã¨ che dobbiamo cambiare il kernel. Quindi per esempio qui, quello che stiamo facendo Ã¨ che stiamo creando una matrice k, dove in ogni entrata, i e j, abbiamo il prodotto di ij per xj. Quindi questo Ã¨ richiesto per fare.

`01:58:03` 
un altro prodotto tra x e se stessa. Quindi se avete x che pensate come un vettore colonna, e avete x che Ã¨ un vettore riga, e entrambi sono x. Queste sono x trasposta. Quindi, se avete X che ha forme in R n per 1, e fate X prodotto matriciale X trasposta, la vostra matrice finale avrÃ  forma in R n per n.

`01:58:41` 
E quello che state facendo Ã¨ che state costruendo una matrice dove ogni NPIJ Ã¨ il prodotto dei due. Quindi, quello che state facendo con il ciclo for qui puÃ² essere fatto scrivendo X per X trasposta piÃ¹ 1 in NPIJ. E questo Ã¨, per esempio, come potete vettorizzare il kernel lineare. Potete fare cose simili anche per il kernel Gaussiano,

`01:59:14` 
e lascio questo a voi come compito, e sarÃ  anche nella soluzione di questo. Ma ricordate che per scrivere un buon codice qui, non dovreste usare un doppio ciclo for, ma usare un'operazione numpy per eseguire questo ciclo for implicitamente usando moltiplicazioni matrice-matrice e questo tipo di operazioni. Okay?

`01:59:45` 
Perfetto. Andiamo all'ultimo notebook per oggi. Quindi ora stiamo parlando di PageRank. Quindi PageRank Ã¨ un algoritmo che Ã¨ stato sviluppato da Google per capire quali nodi di un grafo sono i piÃ¹ importanti.

`02:00:18` 
In questo notebook, quello che ho fatto Ã¨ che ho scritto uno script Python per andare su Wikipedia e fare crawling su alcune pagine sotto la categoria machine learning. E ho creato un grafo che connette ogni pagina a seconda dei link che sono sulle pagine. Quindi per esempio, se vado su Wikipedia e poi vado sulla pagina di machine learning qui, troverete alcuni link.

`02:00:52` 
Per esempio, alla pagina dell'algoritmo statistico. Quindi quello che ho fatto, ho creato uno script Python. crypto che va su queste pagine e crea un arco di un grafo tra due pagine che sono due nodi diversi se c'Ã¨ un link tra loro quindi per esempio abbiamo un arco che va da machine learning a computational statistics inoltre c'Ã¨ un modo anche per ottenere statistiche.

`02:01:23` 
sul traffico web di una certa pagina se andate su wikipedia potete anche fare scraping dei dati che dicono questa pagina ha avuto 1000 utenti questo giorno quindi quello che voglio mostrarvi Ã¨ che se prendiamo questi dati ricostruiamo il grafo delle pagine e calcoliamo con page rank l'importanza delle pagine l'importanza delle pagine correla con le informazioni sul traffico del mondo reale di queste pagine quindi vedrete che se una pagina Ã¨ molto importante usando il page rank riceverÃ  anche molto traffico.

`02:01:58` 
E questa Ã¨ un'applicazione molto utile. Okay? Ãˆ chiaro il contesto? Okay. Quindi qui quello che useremo anche Ã¨ una nuova libreria chiamata NetworkX. Ãˆ un pacchetto Python molto conosciuto che vi aiuta a lavorare con i grafi. Quindi qui la migliore struttura dati che potete usare sono i grafi. Avete familiaritÃ  con i grafi?

`02:02:31` 
SÃ¬? Okay. E NetworkX ci dÃ  molte funzioni e classi da usare in modo efficiente. Quindi prima di tutto, vi do tre file. Carichiamoli perchÃ© non li ho ancora caricati. Che sono edges, nodes, e traffic. Quindi solo per mostrarvi brevemente gli edges sono coppie di pagine.

`02:03:09` 
Quindi per esempio da machine learning potete andare a active learning avete notes che Ã¨ una lista di tutte le pagine e poi avete traffic dove per ogni nodo avete il traffico web per un certo giorno. Okay e questi sono file csv e li leggiamo con pandas.

`02:03:44` 
Quindi quello che abbiamo Ã¨ che abbiamo 250 pagine piÃ¹ o meno 2000 connessioni tra pagine e tante entrate di traffico quanti sono i nodi. La prima cosa che facciamo Ã¨ che costruiamo il grafo usando network links. Quindi questa prima riga, significa che stiamo creando un grafo diretto. Quindi questo significa che gli archi sono orientati.

`02:04:14` 
I link sono orientati perchÃ© potete andare da machine learning ad algoritmi statistici, ma forse sugli algoritmi statistici, non c'Ã¨ il link che vi riporta a machine learning. Quindi questi archi sono diretti perchÃ© non Ã¨ detto che possiate andare da una pagina all'altra se potete andare dalla seconda alla prima. Poi aggiungiamo i nodi leggendo la colonna node dal data frame nodes,

`02:04:47` 
e aggiungiamo gli archi prendendo i valori di source e target. Quindi solo per mostrarvi cosa sta succedendo Ã¨ che abbiamo nodes. che Ã¨ un data frame Pandas, quindi ha questa bella struttura dati, abbiamo una colonna chiamata node, e tutti i valori qui sono le pagine dei valori, e li aggiungiamo. Poi, aggiungiamo gli archi, edges Ã¨ un altro data frame, che ha source e target, quindi noi.

`02:05:22` 
prendiamo queste due colonne, i loro valori, e aggiungiamo gli archi da queste coppie. Con questo, abbiamo creato un grafo, un grafo di networking, che ha alcune utilitÃ  che useremo. Poi, quello che vogliamo avere anche, che sarÃ  utile, Ã¨ che per ora, i nodi sono stringhe.

`02:05:54` 
Vogliamo convertirli in indici. Quindi usando questi due comandi, quello che stiamo facendo Ã¨ che stiamo creando un dizionario che mappa una pagina, quindi una stringa, a un ID. Questo sarÃ  utile quando costruiamo una matrice. Abbiamo visto che dovete costruire una matrice di transizione per calcolare i valori singolari e, scusate, gli autovalori, e quindi il vettore page rank, e quindi per avere l'entrata corrispondente a una certa pagina, usiamo questo dizionario.

`02:06:38` 
Okay, quindi abbiamo il nostro grafo, il nostro dizionario, e ora possiamo costruire questa matrice stocastica M. Quindi M, per ora, Ã¨ una matrice piena di zeri, che ha dimensione, numero. Pagine, per numero di pagine nel nostro dataset. Poi quello che facciamo Ã¨ che possiamo iterare su ogni arco, quindi su ogni link, e questi UMV sono due stringhe, sono le due pagine connesse insieme. Quindi per esempio, qui vedete che Wix supervision Ã¨ connessa a JAX software, e cosÃ¬ via.

`02:07:19` 
Quindi per riempire una matrice, dobbiamo andare, dobbiamo trasformare questa stringa, UMV, nell'indice. Usiamo questo node index, il dizionario che vi ho mostrato prima, per andare dalla stringa, VNU, all'entrata della matrice, ILJ.

`02:07:51` 
Poi sapete che ogni entrata... Ã¨ uno su il numero di archi che vanno da questo nodo perchÃ© quella Ã¨ la probabilitÃ  di saltare da quel nodo a un altro quindi se siete su una pagina che ha 100 link ogni link, abbiamo uno uno su 100 probabilitÃ  di andare da questa pagina a un'altra quindi in un certo senso questa Ã¨ una matrice stocastica che che vi dice qual Ã¨ la probabilitÃ  che l'utente vada da.

`02:08:24` 
una pagina all'altra e all'inizio assumiamo che questa probabilitÃ  sia uniforme quindi la probabilitÃ  di andare da b a u Ã¨ uno sul numero di link che escono da te okay, Questa Ã¨ molta sintassi, ma i concetti non sono troppo difficili.

`02:09:07` 
Una volta che abbiamo M, in realtÃ  non usiamo M, ma usiamo una versione leggermente diversa di M, che potete pensare come una versione regolarizzata di M, che dice che in realtÃ  da una pagina, un utente potrebbe saltare a un'altra pagina casualmente, anche se non Ã¨ collegata ad essa. Ma questa probabilitÃ  Ã¨ davvero piccola. Quindi quello che stiamo aggiungendo Ã¨ questo fattore di smorzamento B, che di solito Ã¨ 85%.

`02:09:41` 
CiÃ² significa che l'utente segue l'85% delle volte i link che sono su quella pagina. Invece, il restante 15% delle volte, puÃ² andare su qualsiasi pagina sul web. in questo caso su qualsiasi pagina che abbiamo scrapato quindi invece di usare m usiamo questa matrice G maiuscola che ci dice che l'85 percento delle volte usiamo la matrice di transizione m invece.

`02:10:12` 
nel 15 percento del tempo rimanente stiamo saltando a caso su qualsiasi pagina quindi questa Ã¨ uh matrice piena di uni ci dice che potete saltare uniformemente su qualsiasi altra pagina dato che Ã¨ piena di uni okay quindi ora ci sono due modi per calcolare il vettore page rank.

`02:10:44` 
Dato che NetworkX Ã¨ una libreria molto usata, abbiamo effettivamente da NetworkX la funzione PageRank che calcola automaticamente il PageRank dei nostri nodi senza che dobbiamo implementarlo. Dobbiamo passare il grafo che stiamo usando e l'alpha, che Ã¨ il fattore di smorzamento. Se date questi due valori a NetworkX, calcola automaticamente il PageRank. Non dovete nemmeno assemblare M e G. NetworkX fa tutto.

`02:11:20` 
Tuttavia, voglio mostrarvi che se usate questa funzione o calcoliamo questo con l'iterazione di potenza, i risultati sono gli stessi. Quindi implementiamo insieme le iterazioni di potenza. E quindi, come facciamo questo? Prima di tutto, dobbiamo definire G maiuscola. Quindi G maiuscola Ã¨ uguale a cosa? A d, il fattore di smorzamento, per m, piÃ¹ 1 meno il fattore di smorzamento, per np.1 di n per n, diviso per N maiuscola. Questa Ã¨ solo la definizione di G.

`02:12:15` 
Poi, le nostre iterazioni. Quindi, il nostro vettore iniziale, p, diciamo che Ã¨ np.1 con dimensione n. e lo dividiamo per n in modo che sia uniforme tutto all'inizio come probabilitÃ  iniziale, quindi potete pensare a questo come invitare come questo vettore come il vettore di probabilitÃ  di essere su ogni pagina e all'inizio diciamo che la probabilitÃ  Ã¨ uniforme poi fissiamo la tolleranza.

`02:12:49` 
diciamo 10 alla meno otto e il numero massimo di iterazioni mille poi iteriamo per i nel range di max iterations cosa facciamo la nostra iterazione quindi il prossimo p p next Ã¨ uguale a g, per p okay l'iterazione di potenza Ã¨ molto semplice applicate solo la matrice g ancora e ancora.

`02:13:24` 
allo stesso vettore poi una volta che l'abbiamo applicato dovete normalizzare quindi p next. Ã¨ uguale a p next diviso per la norma lineare di p next e dopo questo se la differenza nell'iterazione tra p e p next Ã¨ minore della nostra tolleranza abbiamo raggiunto la convergenza.

`02:14:04` 
e usciamo dal ciclo altrimenti il nostro p Ã¨ uguale a p next perchÃ© stiamo continuando il ciclo. Okay, quindi applichiamo la matrice G ancora e ancora alla P finchÃ© la differenza tra due iterazioni successive Ã¨ minore della tolleranza. E questo significa che abbiamo raggiunto la convergenza.

`02:14:47` 
Ultimo passo, vi ho detto che P Ã¨ una probabilitÃ . Quindi nell'ultimo passo, solo per interpretazione, normalizziamo tutto in modo che abbia somma uguale a uno. Quindi P Ã¨ diviso per MP punto somma di P. In questo modo, siamo sicuri che sia una probabilitÃ .

`02:15:33` 
Okay, qualche domanda? Quindi il prossimo passo Ã¨ che confrontiamo il vettore PageRank che abbiamo calcolato con NetworkX e quello che abbiamo calcolato con la PowerIteration.

`02:16:04` 
Okay, quindi quello che stiamo vedendo Ã¨ che correlano davvero bene, quasi uno, e che la loro differenza Ã¨ 10 alla meno 2. Quindi non sono esattamente gli stessi perchÃ© forse li abbiamo implementati in modi leggermente diversi, ma nel complesso hanno lo stesso comportamento. Possiamo anche fare un plot, quindi plt.plot. PageRankVector e PLT.sotPageRankPowering e potete infatti vedere che hanno praticamente.

`02:16:52` 
lo stesso comportamento okay quindi sono leggermente diversi ma piÃ¹ o meno ci stanno raccontando la stessa storia quindi siamo molto felici perchÃ© quello che abbiamo implementato, Ã¨ esattamente uguale a quello che troviamo in una libreria python molto importante.

`02:17:37` 
Okay, il prossimo passo Ã¨ visualizziamo il grafo. Quindi abbiamo questa libreria molto bella, e ora vi mostrerÃ² come possiamo visualizzare la connessione delle pagine in un modo carino. Quindi inizializziamo la figura, poi usiamo questo NetworkX Spring Layout. Questo si chiama Spring Layout perchÃ© i nodi,

`02:18:08` 
la posizione dei nodi Ã¨ simulata come se ogni connessione fosse una stringa, e quindi cerca di trovare il modo migliore per visualizzare il grafo risolvendo un problema fisico dove i nodi sono connessi da stringhe, e di solito questo ci dÃ ... Un bel layout. Tuttavia, dovete impostare K, che Ã¨ la rigiditÃ  della molla e i semi casuali, perchÃ© all'inizio la posizione Ã¨ casuale.

`02:18:42` 
Poi usiamo questa funzione per disegnare i nodi. Quindi passiamo il grafo. Passiamo la posizione che abbiamo simulato con il layout a molla. Definiamo la dimensione dei nodi, il loro colore, la loro trasparenza e altre cose. Poi disegniamo gli archi e infine alcune etichette per capire cos'Ã¨ ogni etichetta. Dato che il grafo Ã¨ molto occupato, dato che abbiamo piÃ¹ di 200 etichette, quello che facciamo Ã¨ che droppiamo solo le etichette per il 10% dei dati piÃ¹ importanti secondo page rank.

`02:19:25` 
Quindi, calcoliamo il quantile del 90% e filtriamo solo i nodi che hanno un'importanza maggiore di questa. Questo richiede un po' di tempo perchÃ© dovete risolvere questa simulazione con il layout a molla e questo Ã¨ il risultato.

`02:19:56` 
Quindi, quello che vedete Ã¨ che le pagine piÃ¹ importanti sono raggruppate. SÃ¬, al centro, hanno forti connessioni. E quindi la simulazione con le molle ha raggruppato tutto proprio qui. Invece, le pagine che hanno meno connessioni non sono connesse nella periferia di questo cluster. Tuttavia, potete vedere anche che alcuni nomi molto, alcuni nomi che sono molto importanti, come cross validation, pattern recognition, statistical learning theory, generative models, e cosÃ¬ via.

`02:20:46` 
Quindi Ã¨ difficile visualizzare bene questo tipo di grafi perchÃ© hanno una struttura molto complessa. Per esempio, possiamo dire che vogliamo solo le etichette piÃ¹ importanti del 5%.

`02:21:16` 
E quindi vedete che, vedete cose che erano precedentemente nascoste dietro, tuttavia, Ã¨ solo per darvi un'idea di cosa sta succedendo. Per esempio, la matrice di confusione che abbiamo visto all'ultimo laboratorio Ã¨ ancora uno degli argomenti molto importanti che trovate sulle pagine di machine learning. Okay, ultimo passo, ultimo sforzo per oggi, Ã¨ che ora abbiamo il vettore page ranking che ci dice le pagine piÃ¹ importanti.

`02:21:53` 
In particolare, i nodi piÃ¹ importanti, e vogliamo confrontare quello che abbiamo calcolato con il page rank e i dati del traffico del mondo reale. Quindi abbiamo in questo data frame del traffico. il traffico che alcune pagine hanno avuto durante alcuni giorni questa settimana. Con questo comando, aggiungiamo a questo data frame anche una colonna con i nodi. Quindi vedete che su ogni riga.

`02:22:27` 
abbiamo il nome del nodo e il traffico che ha ricevuto. E aggiungiamo anche il page rate. Quindi se eseguiamo questa cella ora, abbiamo una colonna in piÃ¹ dove abbiamo il traffico e la probabilitÃ  del page rate di essere su quella pagina. Ora possiamo fare un plot. Quindi plt.scatter. E quello che facciamo Ã¨ che tracciamo il traffic.traffic.

`02:23:01` 
che sono i dati del traffico e traffic.pageRate. Uh, che Ã¨ importante. Quindi se vediamo questo qui, uh, sembra che non siano molto correlati. Uh, tuttavia, se cambiamo l'asse logaritmico, potete vedere che c'Ã¨ una sorta di tendenza. Okay. Uh, il nostro traffico funziona molto, Ã¨ molto complesso. Uh, non potete immaginare che un algoritmo semplice come questo catturi davvero il comportamento di questo complesso.

`02:23:59` 
Tuttavia, c'Ã¨ una sorta di linea di tendenza che vi mostra che in realtÃ  c'Ã¨ una certa correlazione tra, uh, il page rank, uh, vettore e questo traffico. Per mostrarvi alcuni dati piÃ¹ quantitativi, calcoleremo ora il coefficiente di correlazione. Quindi da numpy, potete usare la funzione corcoef che calcola il coefficiente di correlazione.

`02:24:30` 
e passeremo queste due variabili. Se stampiamo questo, vedete che abbiamo una correlazione del 66%. Non Ã¨ molto grande, ma questo Ã¨ molto piÃ¹ grande di qualcosa che succede a caso, okay? Quindi.

`02:25:02` 
Per me, questo mostra che in realtÃ  questo algoritmo sta vedendo qualcosa all'interno dei dati e sta facendo alcune previsioni ragionevoli sul traffico delle varie pagine. Solo per martellare questo punto, una cosa che potreste fare Ã¨ dire, cosa succederebbe se invece di usare PageRank, facessimo questa previsione a caso? PerchÃ© il predittore casuale Ã¨ spesso usato come baseline. Quindi quello che possiamo fare Ã¨ che invece di usare questo, mescoliamo i dati.

`02:26:01` 
E se applichiamo una permutazione casuale a questi dati, quindi invece di usare effettivamente il page rank, diciamo che questo valore Ã¨ calcolato a caso, vedete che la correlazione Ã¨ sempre davvero vicina a zero. Okay, ogni volta che eseguite questa cella, state applicando una permutazione diversa. E quindi se state solo, diciamo, calcolando a caso l'importanza delle pagine, vedete che la correlazione Ã¨ sempre davvero vicina a zero. Invece, usando questo algoritmo, abbiamo una correlazione di 0.6, che non Ã¨ grande, ma Ã¨ significativa nel dire che questo algoritmo sta facendo qualcosa di buono.

`02:26:44` 
Okay, avete qualche domanda? Se no, Ã¨ tutto per oggi. Per qualsiasi domanda, se avete qualcosa in mente, potete scrivermi una email, potete trovarmi qui, e buon fine settimana.