## Focus e ambito del corso
- Approfondimento di algebra lineare e algebra lineare numerica con enfasi su applicazioni in machine learning e data science.
- Famiglie di problemi trattate:
  - Sistemi lineari AX = B: condizioni di risolvibilitÃ  e relazione con gli spazi fondamentali.
  - Decomposizione spettrale: AX = Î»X; interpretazione geometrica degli autovettori (direzione preservata, lunghezza scalata).
  - Decomposizione ai valori singolari (SVD): AV = ÏƒU; vettori singolari (V, U) e valori singolari (Ïƒ); applicabile a qualsiasi matrice, anche singolare/non quadrata; considerata una generalizzazione della decomposizione spettrale ed essenziale per data science/ML.
  - Minimizzazione dei minimi quadrati: regressione collegata agli strumenti di algebra lineare.
  - Fattorizzazioni di matrici: fattorizzazioni didattiche e pratiche; ordine di trattazione flessibile (sistemi â†’ fattorizzazioni â†’ autovalori/SVD â†’ minimizzazione).
## Rappresentazione dei dati con matrici
- Qualsiasi dataset puÃ² essere rappresentato come una matrice A âˆˆ R^{mÃ—n}:
  - m = numero di campioni (righe).
  - n = numero di feature (colonne).
- Esempio:
  - Dataset di migliaia di immagini in scala di grigi, ciascuna 1000Ã—1000 pixel.
  - Ogni immagine â†’ vettore di lunghezza 1.000.000 (intensitÃ  pixel 0â€“255).
  - Dataset completo â†’ matrice con migliaia di righe e 1.000.000 colonne.
- Le feature non numeriche possono essere codificate numericamente (ad esempio, one-hot encoding) cosÃ¬ le matrici possono rappresentare qualsiasi dataset.
## Moltiplicazione matrice-vettore: prospettiva dello spazio delle colonne
- L'operazione AX produce un vettore che Ã¨ una combinazione lineare delle colonne di A:
  - AX = x1Â·a1 + x2Â·a2 + â€¦, dove ai sono le colonne di A.
- Spazio delle colonne C(A):
  - Insieme di tutte le combinazioni lineari delle colonne di A.
  - Il risultato di AX giace sempre in C(A).
### Esempi e interpretazione geometrica
- A1 (3Ã—2) con colonne a1 = [1,2,1]^T, a2 = [3,1,-1]^T:
  - AX = x1Â·a1 + x2Â·a2 â†’ C(A1) Ã¨ un piano passante per l'origine (rango 2).
- A2 (3Ã—3) = [[1,2,3],[4,5,6],[7,8,9]]:
  - a3 = 2Â·a2 âˆ’ a1 â†’ non indipendente; C(A2) coincide con C(A2â€²) dove A2â€² = [[1,2],[4,5],[7,8]].
  - C(A2) Ã¨ un piano (rango 2), sottospazio di R^3; a3 giace nel piano.
- A3 (3Ã—3) = [[1,2,3],[4,5,6],[7,8,10]]:
  - Tre colonne indipendenti â†’ C(A3) = R^3 (rango 3).
- A4 (3Ã—3) = [[1,2,3],[2,4,6],[3,6,9]]:
  - a2 = 2Â·a1, a3 = 3Â·a1 â†’ solo una colonna indipendente.
  - C(A4) Ã¨ una retta passante per l'origine con direzione [1,2,3]^T (rango 1).
## Rango: definizione ed esempi
- Rango r(A):
  - Numero di colonne linearmente indipendenti.
  - Coincide con la dimensione dello spazio delle colonne C(A).
- Esempi di rango:
  - r(A1) = 2, r(A2) = 2, r(A2â€²) = 2, r(A3) = 3, r(A4) = 1.
## RisolvibilitÃ  dei sistemi lineari AX = B
- AX = B ha soluzione se e solo se B âˆˆ C(A).
  - PoichÃ© AX âˆˆ C(A) per ogni X; se B âˆ‰ C(A), l'uguaglianza Ã¨ impossibile.
- Implicazione pratica: verificare se B giace nello spazio delle colonne determina la risolvibilitÃ .
## Costruzione di una base per lo spazio delle colonne e fattorizzazione CR
- Procedura per trovare una base di C(A):
  - Inizia con c1 = a1 (prima colonna).
  - Per ogni colonna successiva ak:
    - Se ak Ã¨ proporzionale a un vettore di base esistente (o combinazione lineare della base attuale), scarta.
    - Altrimenti, aggiungi ak alla base.
- Esempio con A2 = [[1,2,3],[4,5,6],[7,8,9]]:
  - Vettori di base: a1, a2 (a3 = 2Â·a2 âˆ’ a1).
  - r(A2) = 2.
### Fattorizzazione CR (prospettiva forma ridotta per righe)
- Costruisci C selezionando colonne indipendenti: C = [a1 a2] = [[1,2],[4,5],[7,8]] (3Ã—2).
- Trova R (2Ã—3) tale che CÂ·R = A:
  - Le colonne di R contengono i coefficienti per ricostruire ogni colonna originale come combinazione lineare delle colonne di C:
    - r1 = [1,0]^T â†’ CÂ·r1 = a1.
    - r2 = [0,1]^T â†’ CÂ·r2 = a2.
    - r3 = [âˆ’1,2]^T â†’ CÂ·r3 = a3 = 2Â·a2 âˆ’ a1.
- A2 = CÂ·R fornisce una fattorizzazione didattica; si collega alle forme ridotte per righe (rref).
- Note:
  - Se A ha rango massimo per colonne (ad esempio, A3), allora C = A e R = I.
  - Le colonne di C non sono ortogonali o normalizzate; utilitÃ  pratica limitata ma chiarisce la struttura.
## Invarianza del rango rispetto alla trasposizione
- Per A2â€² = [[1,2,3],[4,5,6]] (3Ã—2), considera A2â€²^T = [[1,4],[2,5],[3,6]]:
  - Terza colonna uguale a 2Â·seconda âˆ’ prima; rango = 2.
- Risultato generale: rango(A^T) = rango(A); dim C(A^T) = dim C(A).
## Moltiplicazione matrice-matrice: decomposizione colonna-riga (prodotto esterno)
- Moltiplicazione standard: Se A âˆˆ R^{mÃ—n}, B âˆˆ R^{nÃ—p}, allora AB âˆˆ R^{mÃ—p}, calcolata tramite prodotti riga per colonna.
- Vista colonna-riga:
  - Dividi A nelle sue colonne {cA1, cA2, â€¦, cAn}.
  - Dividi B nelle sue righe {rB1, rB2, â€¦, rBn}.
  - AB = Î£_{k=1}^n (cAk Â· rBk), dove ogni termine Ã¨ un prodotto esterno (mÃ—1 per 1Ã—p â†’ mÃ—p).
- Ogni termine prodotto esterno Ã¨ di rango 1 per costruzione; il prodotto completo Ã¨ una somma di contributi di rango 1.
- Importanza concettuale:
  - Forma la base per rappresentare matrici come somme di componenti di rango 1.
  - Si collega direttamente a SVD e PCA.
  - PCA si basa su approssimazioni a basso rango derivate da SVD, rendendo centrale la visione della somma di rango 1.
### Esempio e contributi di rango 1
- Esempio: A = [[1,2],[3,4]], B = [[2,1],[2,3]].
  - cA1Â·rB1 = [1,3]^T Â· [2,1] = [[2,1],[6,3]], rango 1.
  - cA2Â·rB2 = [2,4]^T Â· [2,3] = [[4,6],[8,12]], rango 1.
  - Somma = [[6,7],[14,15]], identica al risultato della moltiplicazione standard.
- Intuizione:
  - AB Ã¨ una somma di matrici di rango 1; fondamentale per SVD e approssimazioni a basso rango.
## Sottospazi fondamentali, ortogonalitÃ  e dimensioni
- Dato A âˆˆ R^{mÃ—n} con rango(A) = r.
- Spazi delle colonne:
  - Col(A) âŠ‚ R^m, dim(Col(A)) = r.
  - Col(A^T) âŠ‚ R^n, dim(Col(A^T)) = r.
- Nuclei (= null spaces):
  - Null(A) âŠ‚ R^n (vettori x con A x = 0).
  - Null(A^T) âŠ‚ R^m (vettori y con A^T y = 0).
- Relazioni di ortogonalitÃ :
  - Col(A^T) âŸ‚ Null(A): ogni x âˆˆ Null(A) Ã¨ ortogonale a ogni riga di A.
  - Col(A) âŸ‚ Null(A^T): ogni y âˆˆ Null(A^T) Ã¨ ortogonale a ogni colonna di A.
- Complementi ortogonali negli spazi ambienti:
  - In R^m: Col(A) e Null(A^T) sono complementi ortogonali; dim(Col(A)) = r, dim(Null(A^T)) = m âˆ’ r.
  - In R^n: Col(A^T) e Null(A) sono complementi ortogonali; dim(Col(A^T)) = r, dim(Null(A)) = n âˆ’ r.
- Teorema rango-nullo:
  - dim(Null(A)) = n âˆ’ r e dim(Null(A^T)) = m âˆ’ r.
### Interpretazione di A x = 0 tramite prodotti scalari
- Matrice di esempio: A = [[1,2,3],[4,5,6],[7,8,9]], x = [x1; x2; x3], condizione A x = 0.
- Vista moltiplicazione rigaâ€“colonna:
  - Siano r1, r2, r3 le righe di A.
  - A x = 0 implica r_i Â· x = 0 per i = 1,2,3 (prodotti scalari).
- Implicazioni:
  - x Ã¨ ortogonale a tutte le righe di A; quindi x âˆˆ Null(A) â‡’ x âŸ‚ Col(A^T).
### ProprietÃ  di sottospazio dei nuclei
- Chiusura e scalabilitÃ  del nucleo:
  - Il vettore 0 Ã¨ in Null(A).
  - Se x, y âˆˆ Null(A), allora x + y âˆˆ Null(A).
  - Se x âˆˆ Null(A) e Î± âˆˆ R, allora Î±x âˆˆ Null(A).
### Base costruttiva per Null(A) tramite fattorizzazione a blocchi
- Setup: A âˆˆ R^{mÃ—n}, rango(A) = r.
- Partiziona A in A1 (mÃ—r) con colonne linearmente indipendenti e A2 (mÃ—(nâˆ’r)) le restanti colonne dipendenti.
- Esprimi A2 come combinazione lineare di A1: A2 = A1 Â· B, con B âˆˆ R^{rÃ—(nâˆ’r)}.
- Quindi A = [A1, A1 B].
- Costruisci K âˆˆ R^{nÃ—(nâˆ’r)}: K = [âˆ’B; I_{nâˆ’r}].
-- Calcola A K: A K = [A1, A1 B] [âˆ’B; I] = A1(âˆ’B) + A1 B = 0 â‡’ le colonne di K giacciono in Null(A) e sono linearmente indipendenti.
-- Qualsiasi U âˆˆ Null(A) puÃ² essere scritto come U = K U2:
  - Partiziona U = [U1; U2]; AU = 0 â‡’ A1(U1 + B U2) = 0 â‡’ U1 = âˆ’B U2.
  - Quindi U = [âˆ’B U2; U2] = K U2.
-- Conclusione:
  - Le colonne di K formano una base per Null(A); dim Null(A) = n âˆ’ r.
  - Ragionamento simmetrico porta a dim Null(A^T) = m âˆ’ r.
## Matrici ortogonali, proiezioni e geometria
- Matrice ortogonale Q: Q^T Q = I; det(Q) = Â±1.
- Conservazione della norma:
  - Per Y = QX: ||Y||^2 = X^T Q^T Q X = ||X||^2; le trasformazioni ortogonali sono rigide (preservano la lunghezza).
- Rotazione 2D:
  - R(Î¸) = [[cos Î¸, âˆ’sin Î¸],[sin Î¸, cos Î¸]]; ortogonale, det = +1; ruota di Î¸.
- Riflesso rispetto a un piano Î  con normale unitaria n:
  - v_âŠ¥ = (v Â· n) n; riflesso w = v âˆ’ 2(v Â· n) n.
  - Matrice: R_ref = I âˆ’ 2 n n^T; ortogonale, det = âˆ’1; R_ref^{-1} = R_ref.
- Proiezione ortogonale su Î :
  - P = I âˆ’ n n^T; singolare (det = 0); non invertibile per perdita di informazione.
- Chiarimenti sulle proiezioni:
  - Proiezione di a sulla direzione b (non unitaria): vettore proiettato = (a Â· b) (b / ||b||^2).
## Collegamenti con machine learning e data science
- SVD:
  - Decompone A come Î£_i Ïƒ_i u_i v_i^T (somma di matrici di rango 1); applicabilitÃ  universale; alla base di PCA, riduzione dimensionale, filtraggio del rumore e modellazione a basso rango.
- PCA:
  - Sfrutta i valori/vettori singolari dominanti per approssimazioni a basso rango che catturano la varianza e riducono la dimensionalitÃ .
- Minimi quadrati:
  - Regressione vista come minimizzazione dei residui; la risolvibilitÃ  Ã¨ legata allo spazio delle colonne e al rango (equazioni normali, pseudoinversa).
- Rango e spazio delle colonne:
  - Determinano l'identificabilitÃ  del modello, la ridondanza delle feature e le condizioni di risolvibilitÃ  per AX = B.
- Prospettiva colonna-riga:
  - Fornisce comprensione strutturale dei prodotti e motiva le approssimazioni a rango vincolato usate in ML.
- Collegamento con metodi numerici:
  - Riflessi di Householder (da matrici di riflessione) per fattorizzazione QR.
  - Rotazioni di Givens (da matrici di rotazione) per calcoli di autovalori/QR.
## Fatti chiave e conclusioni
- I prodotti tra matrici si decompongono in somme di matrici di rango 1; fondamentale per SVD/PCA.
- Quattro sottospazi fondamentali e dimensioni:
  - Col(A) âŠ‚ R^m, dim r; Col(A^T) âŠ‚ R^n, dim r.
  - Null(A^T) âŠ‚ R^m, dim m âˆ’ r; Null(A) âŠ‚ R^n, dim n âˆ’ r.
- Relazioni ortogonali:
  - Col(A) âŸ‚ Null(A^T) in R^m; Col(A^T) âŸ‚ Null(A) in R^n.
  - Ogni coppia forma complementi ortogonali negli spazi ambienti.
- Metodo costruttivo con K = [âˆ’B; I_{nâˆ’r}] fornisce una base per Null(A).
- Le matrici di proiezione sono singolari; riflessioni e rotazioni sono ortogonali con det Â±1.
## Esempi e contabilitÃ  dimensionale
- Esempio numerico: A = [[1,2,3],[4,5,6],[7,8,9]] mostra che i vettori di Null(A) sono ortogonali alle righe.
- Partizionamento delle dimensioni:
  - A1: mÃ—r; A2: mÃ—(nâˆ’r); B: rÃ—(nâˆ’r); K: nÃ—(nâˆ’r); A K: mÃ—(nâˆ’r) zero.
## ðŸ“… Prossimi passi e attivitÃ  da svolgere
- [ ] Ripassare i sistemi lineari AX = B con attenzione al criterio di risolvibilitÃ  B âˆˆ C(A).
- [ ] Costruire basi per gli spazi delle colonne usando controlli di indipendenza su matrici di esempio (A1, A2, A3, A4).
- [ ] Esercitarsi con la fattorizzazione CR: formare C dalle colonne indipendenti e calcolare R per ricostruire A.
- [ ] Verificare rango(A^T) = rango(A) su altri esempi di matrici.
- [ ] Rielaborare esempi di moltiplicazione matrice-matrice usando la vista colonna-riga (prodotto esterno) e identificare componenti di rango 1.
- [ ] Completare e interiorizzare la dimostrazione che le colonne di K generano Null(A); formalizzare dim Null(A) = n âˆ’ r e dim Null(A^T) = m âˆ’ r.
- [ ] Prepararsi ai prossimi moduli: decomposizione spettrale, fondamenti e applicazioni SVD, minimizzazione dei minimi quadrati e fattorizzazione QR tramite riflessioni di Householder e rotazioni di Givens.
- [ ] Convertire feature non numeriche in rappresentazioni numeriche (ad esempio, one-hot) per dataset basati su matrici.
- [ ] Esplorare le implicazioni pratiche di rango e spazio delle colonne nel preprocessing dei dati (ridondanza delle feature, dimensionalitÃ ).
- [ ] Partecipare al prossimo incontro di venerdÃ¬; riprendere dopo una breve pausa alle 15:00.